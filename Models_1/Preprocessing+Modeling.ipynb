{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      NDF\n",
      "1      NDF\n",
      "2       US\n",
      "3    other\n",
      "4       US\n",
      "Name: country_destination, dtype: object\n",
      "           id date_account_created  timestamp_first_active date_first_booking  \\\n",
      "0  gxn3p5htnn           2010-06-28          20090319043255                NaN   \n",
      "1  820tgsjxq7           2011-05-25          20090523174809                NaN   \n",
      "2  4ft3gnwmtx           2010-09-28          20090609231247         2010-08-02   \n",
      "3  bjjt8pjhuk           2011-12-05          20091031060129         2012-09-08   \n",
      "4  87mebub9p4           2010-09-14          20091208061105         2010-02-18   \n",
      "\n",
      "      gender  age signup_method  signup_flow language affiliate_channel  \\\n",
      "0  -unknown-  NaN      facebook            0       en            direct   \n",
      "1       MALE   38      facebook            0       en               seo   \n",
      "2     FEMALE   56         basic            3       en            direct   \n",
      "3     FEMALE   42      facebook            0       en            direct   \n",
      "4  -unknown-   41         basic            0       en            direct   \n",
      "\n",
      "  affiliate_provider first_affiliate_tracked signup_app first_device_type  \\\n",
      "0             direct               untracked        Web       Mac Desktop   \n",
      "1             google               untracked        Web       Mac Desktop   \n",
      "2             direct               untracked        Web   Windows Desktop   \n",
      "3             direct               untracked        Web       Mac Desktop   \n",
      "4             direct               untracked        Web       Mac Desktop   \n",
      "\n",
      "  first_browser  \n",
      "0        Chrome  \n",
      "1        Chrome  \n",
      "2            IE  \n",
      "3       Firefox  \n",
      "4        Chrome  \n"
     ]
    }
   ],
   "source": [
    "# Read the train and the test data \n",
    "train_users = pd.read_csv('train_users_2.csv')\n",
    "test_users = pd.read_csv('test_users.csv')\n",
    "\n",
    "\n",
    "# Extracting labels from the train data\n",
    "train_users_labels = train_users.loc[:,'country_destination']\n",
    "print (train_users_labels.head(n=5))\n",
    "\n",
    "# Extracting attributes from the train data\n",
    "train_users_attrs = train_users.iloc[:,0:15]\n",
    "print(train_users_attrs.head(n=5))\n",
    "\n",
    "train_users = train_users_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_users = train_users.drop(['date_first_booking'], axis=1)\n",
    "test_users = test_users.drop(['date_first_booking'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date is split into 3 parts as year, month and day in both test and train. These are added as\n",
    "# new features in both test and train\n",
    "\n",
    "date_acc_created = np.vstack(train_users.date_account_created.astype(str).apply(\n",
    "        lambda x: list(map(int, x.split('-')))).values)\n",
    "train_users['created_year'] = date_acc_created[:,0]\n",
    "train_users['created_month'] = date_acc_created[:,1]\n",
    "train_users['created_day'] = date_acc_created[:,2]\n",
    "train_users = train_users.drop(['date_account_created'], axis=1)\n",
    "\n",
    "date_acc_created_test = np.vstack(test_users.date_account_created.astype(str).apply(\n",
    "        lambda x: list(map(int, x.split('-')))).values)\n",
    "test_users['created_year'] = date_acc_created_test[:,0]\n",
    "test_users['created_month'] = date_acc_created_test[:,1]\n",
    "test_users['created_day'] = date_acc_created_test[:,2]\n",
    "test_users = test_users.drop(['date_account_created'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing unknown values in gender with -1 and null values with -1\n",
    "train_users.loc[ train_users['gender'] == '-unknown-', 'gender'] = -1\n",
    "train_users.loc[ train_users['gender'].isnull(), 'gender' ] = -1\n",
    "test_users.loc[ test_users['gender'] == '-unknown-', 'gender'] = -1\n",
    "test_users.loc[ test_users['gender'].isnull(), 'gender'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding Female with 0, Male with 1 and Other with 2 in both test and train data\n",
    "gender_translation = {'FEMALE' : 0,\n",
    "                     'MALE' : 1,\n",
    "                     'OTHER' : 2,\n",
    "                     -1 : -1 }\n",
    "for data in [train_users, test_users]:\n",
    "    data['gender'] = data['gender'].apply(lambda x: gender_translation[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing gender value distribution\n",
      "(0, ':', 0.5353209412124351)\n",
      "(1, ':', 0.46228441870536585)\n",
      "(2, ':', 0.002394640082198993)\n"
     ]
    }
   ],
   "source": [
    "# Finding valid values for gender and invalid values for gender\n",
    "nan_gender_count = len(train_users.loc[train_users['gender'] == -1, 'gender'])\n",
    "valid_gender_count = len(train_users.gender.values) - nan_gender_count\n",
    "\n",
    "# Creating a map with the gender distribution\n",
    "count_map = pd.value_counts(train_users['gender'].values)\n",
    "print (\"Existing gender value distribution\")\n",
    "for k, v in count_map.iteritems():\n",
    "    if k == -1:\n",
    "        continue\n",
    "    print (k, \":\", float(v)/float(valid_gender_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making the gender distribution the same for missing imputation\n",
    "for k, v in count_map.iteritems():\n",
    "    if k == -1:\n",
    "        continue\n",
    "    c = int ( nan_gender_count * float(v)/float(valid_gender_count) )\n",
    "    for i in range(len(train_users.gender.values)):\n",
    "        if train_users.gender.values[i] == -1:\n",
    "            train_users.gender.values[i] = k\n",
    "            c -= 1\n",
    "        if c == 0:\n",
    "            break\n",
    "train_users.gender.values[213450] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    213451.000000\n",
       "mean          0.467072\n",
       "std           0.503691\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           1.000000\n",
       "max           2.000000\n",
       "Name: gender, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_users.gender.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing gender value distribution\n",
      "(0, ':', 0.5116944601469757)\n",
      "(1, ':', 0.486468343697004)\n",
      "(2, ':', 0.0018371961560203504)\n"
     ]
    }
   ],
   "source": [
    "nan_gender_count = len(test_users.loc[test_users['gender'] == -1, 'gender'])\n",
    "valid_gender_count = len(test_users.gender.values) - nan_gender_count\n",
    "count_map = pd.value_counts(test_users['gender'].values)\n",
    "print (\"Existing gender value distribution\")\n",
    "for k, v in count_map.iteritems():\n",
    "    if k == -1:\n",
    "        continue\n",
    "    print (k, \":\", float(v)/float(valid_gender_count))\n",
    "\n",
    "for k, v in count_map.iteritems():\n",
    "    if k == -1:\n",
    "        continue\n",
    "    c = int ( nan_gender_count * float(v)/float(valid_gender_count) )\n",
    "    for i in range(len(test_users.gender.values)):\n",
    "        if test_users.gender.values[i] == -1:\n",
    "            test_users.gender.values[i] = k\n",
    "            c -= 1\n",
    "        if c == 0:\n",
    "            break\n",
    "test_users.gender.values[62094] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    125461.000000\n",
       "mean         49.668335\n",
       "std         155.666612\n",
       "min           1.000000\n",
       "25%          28.000000\n",
       "50%          34.000000\n",
       "75%          43.000000\n",
       "max        2014.000000\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_users['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing invalid age with NaN in test and train\n",
    "\n",
    "train_users.loc[train_users['age'] > 95, 'age'] = np.nan\n",
    "train_users.loc[train_users['age'] < 16, 'age'] = np.nan\n",
    "test_users.loc[test_users['age'] > 95, 'age'] = np.nan\n",
    "test_users.loc[test_users['age'] < 16, 'age'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.0\n",
      "31.0\n"
     ]
    }
   ],
   "source": [
    "# Replace missing age with median\n",
    "print (train_users.age.median())\n",
    "print (test_users.age.median())\n",
    "train_users.loc[ train_users['age'].isnull(), 'age' ] = train_users.age.median()\n",
    "test_users.loc[ test_users['age'].isnull(), 'age' ] = test_users.age.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding the signup method for test\n",
    "signup_translation = {'facebook' : 0,\n",
    "                     'google' : 1,\n",
    "                     'basic' : 2,\n",
    "                     'weibo' : 3}\n",
    "for data in [train_users, test_users]:\n",
    "    data['signup_method'] = data['signup_method'].apply(lambda x: signup_translation[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding the language in both train and test\n",
    "test_users.loc[ test_users['language'] == '-unknown-', 'language'] = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_encoding = {'en'      :       1       ,\n",
    "'zh'      :       2       ,\n",
    "'fr'      :       3       ,\n",
    "'es'      :       4       ,\n",
    "'ko'      :       5       ,\n",
    "'de'      :       6       ,\n",
    "'it'      :       7       ,\n",
    "'ru'      :       8       ,\n",
    "'pt'      :       9       ,\n",
    "'ja'      :       10      ,\n",
    "'sv'      :       11      ,\n",
    "'nl'      :       12      ,\n",
    "'tr'      :       13      ,\n",
    "'da'      :       14      ,\n",
    "'pl'      :       15      ,\n",
    "'cs'      :       16      ,\n",
    "'no'      :       17      ,\n",
    "'el'      :       18      ,\n",
    "'th'      :       19      ,\n",
    "'id'      :       20      ,\n",
    "'hu'      :       21      ,\n",
    "'fi'      :       22      ,\n",
    "'ca'      :       23      ,\n",
    "'is'      :       24      ,\n",
    "'hr'      :       25}\n",
    "\n",
    "for data in [train_users, test_users]:\n",
    "    data['language'] = data['language'].apply(lambda x: language_encoding[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding for affiliate_channel\n",
    "affiliate_channel_encoding = {'direct' : 1,\n",
    "                             'sem-brand' : 2,\n",
    "                             'sem-non-brand' : 3,\n",
    "                             'other' : 4,\n",
    "                             'api' : 5,\n",
    "                             'seo' : 6,\n",
    "                             'content' : 7,\n",
    "                             'remarketing' : 8}\n",
    "for data in [train_users, test_users]:\n",
    "    data['affiliate_channel'] = data['affiliate_channel'].apply(lambda x: affiliate_channel_encoding[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding for affiliate_provider\n",
    "affiliate_provider_encoding = {'direct':1,\n",
    "'google':2,\n",
    "'other':3,\n",
    "'craigslist':4,\n",
    "'bing':5,\n",
    "'facebook':6,\n",
    "'vast':7,\n",
    "'padmapper':8,\n",
    "'facebook-open-graph':9,\n",
    "'yahoo':10,\n",
    "'gsp':11,\n",
    "'meetup':12,\n",
    "'email-marketing':13,\n",
    "'naver':14,\n",
    "'baidu':15,\n",
    "'yandex':16,\n",
    "'wayn':17,\n",
    "'daum':18}\n",
    "\n",
    "for data in [train_users, test_users]:\n",
    "    data['affiliate_provider'] = data['affiliate_provider'].apply(lambda x: affiliate_provider_encoding[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding for first_affiliate_tracked\n",
    "train_users.loc[ train_users['first_affiliate_tracked'].isnull(), 'first_affiliate_tracked'] = \"untracked\"\n",
    "test_users.loc[ test_users['first_affiliate_tracked'].isnull(), 'first_affiliate_tracked'] = \"untracked\"\n",
    "first_affiliate_tracked_encoding = {'untracked' : 1,\n",
    "                                   'linked' : 2,\n",
    "                                   'omg' : 3,\n",
    "                                   'tracked-other' : 4,\n",
    "                                   'product' : 5,\n",
    "                                   'marketing' : 6,\n",
    "                                   'local ops' : 7}\n",
    "for data in [train_users, test_users]:\n",
    "    data['first_affiliate_tracked'] = data['first_affiliate_tracked'].apply(lambda x: first_affiliate_tracked_encoding[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding for signup_app\n",
    "signup_app_encoding = {'Web' : 1,\n",
    "                      'iOS' : 2,\n",
    "                      'Android' : 3,\n",
    "                      'Moweb' : 4}\n",
    "for data in [train_users, test_users]:\n",
    "    data['signup_app'] = data['signup_app'].apply(lambda x: signup_app_encoding[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding for first_device_type\n",
    "first_device_type_encoding = { 'Mac Desktop' : 1,\n",
    "                             'iPhone' : 2,\n",
    "                             'Windows Desktop' : 3,\n",
    "                             'Android Phone' : 4,\n",
    "                             'iPad' : 5,\n",
    "                             'Android Tablet' : 6,\n",
    "                             'Other/Unknown' : 7,\n",
    "                             'Desktop (Other)' : 8,\n",
    "                             'SmartPhone (Other)' : 9}\n",
    "for data in [train_users, test_users]:\n",
    "    data['first_device_type'] = data['first_device_type'].apply(lambda x: first_device_type_encoding[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoding for first_browser\n",
    "first_browser_encoding = {'Chrome':1,\n",
    "'Safari':2,\n",
    "'Firefox':3,\n",
    "'-unknown-':4,\n",
    "'IE':5,\n",
    "'Mobile Safari':6,\n",
    "'Chrome Mobile':7,\n",
    "'Android Browser':8,\n",
    "'AOL Explorer':9,\n",
    "'Opera':10,\n",
    "'Silk':11,\n",
    "'Chromium':12,\n",
    "'BlackBerry Browser':13,\n",
    "'Maxthon':14,\n",
    "'IE Mobile':15,\n",
    "'Apple Mail':16,\n",
    "'Sogou Explorer':17,\n",
    "'Mobile Firefox':18,\n",
    "'RockMelt':19,\n",
    "'SiteKiosk':20,\n",
    "'Iron':21,\n",
    "'IceWeasel':22,\n",
    "'Pale Moon':23,\n",
    "'SeaMonkey':24,\n",
    "'Yandex.Browser':25,\n",
    "'CometBird':26,\n",
    "'Camino':27,\n",
    "'TenFourFox':28,\n",
    "'wOSBrowser':29,\n",
    "'CoolNovo':30,\n",
    "'Avant Browser':31,\n",
    "'Opera Mini':32,\n",
    "'Mozilla':33,\n",
    "'Comodo Dragon':34,\n",
    "'TheWorld Browser':35,\n",
    "'Crazy Browser':36,\n",
    "'Flock':37,\n",
    "'OmniWeb':38,\n",
    "'SlimBrowser':39,\n",
    "'Opera Mobile':40,\n",
    "'Conkeror':41,\n",
    "'Outlook 2007':42,\n",
    "'Palm Pre web browser':43,\n",
    "'Stainless':44,\n",
    "'NetNewsWire':45,\n",
    "'Kindle Browser':46,\n",
    "'Epic':47,\n",
    "'Googlebot':48,\n",
    "'Arora':49,\n",
    "'Google Earth':50,\n",
    "'IceDragon':51,\n",
    "'PS Vita browser':52,\n",
    "'IBrowse' : 53,\n",
    "'UC Browser' : 54,\n",
    "'IBrowse': 55,\n",
    "'Nintendo Browser' : 56}\n",
    "\n",
    "\n",
    "for data in [train_users, test_users]:\n",
    "    data['first_browser'] = data['first_browser'].apply(lambda x: first_browser_encoding[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading sessions data\n",
    "sessions = pd.read_csv('sessions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135483,)\n",
      "mxqbh3ykxl    2722\n",
      "0hjoc5q8nf    2644\n",
      "mjbl6rrj52    2476\n",
      "l5lgm3w5pc    2424\n",
      "wg9413iaux    2362\n",
      "ht8alhs4lt    2335\n",
      "wyv1imf8qw    2323\n",
      "monrpvx2md    2264\n",
      "9z4gim1s4l    2264\n",
      "h0cjxc177k    2246\n",
      "a0uhiojrra    2137\n",
      "vcmr2jh5ix    2085\n",
      "1m6xnhstmb    2019\n",
      "p1183hxzc4    1938\n",
      "e8h4qghxlg    1923\n",
      "gey51ednme    1919\n",
      "5vpuk5mssg    1876\n",
      "j2cvctvqve    1861\n",
      "yu5bdalz2b    1811\n",
      "ejpe95pcyo    1797\n",
      "r541x78s24    1792\n",
      "qkbkunyzq7    1780\n",
      "n4s6g3grzf    1779\n",
      "bfiueza7rt    1753\n",
      "b1io359wpg    1752\n",
      "8ikl7vnfa3    1732\n",
      "e81qfos71y    1701\n",
      "s5ez13snz0    1685\n",
      "93dulcecw0    1614\n",
      "r0rgjqbsvp    1612\n",
      "              ... \n",
      "vlji8fg52x       1\n",
      "4s2v2hmngj       1\n",
      "n2rrpf1t3h       1\n",
      "ua4bebdziw       1\n",
      "gks02el96u       1\n",
      "e7l7yocdtk       1\n",
      "ztvrwgyxm2       1\n",
      "w5sn4qqiav       1\n",
      "9o5gi1x2i4       1\n",
      "kl81vani0y       1\n",
      "1uaksuktr5       1\n",
      "c9vanbl9nh       1\n",
      "n6tcyc7thd       1\n",
      "cgdsmvs4sw       1\n",
      "f9ohif5u6w       1\n",
      "wiru94r12h       1\n",
      "l28osl4y6x       1\n",
      "t9o5rwmg1k       1\n",
      "hjhljq8k89       1\n",
      "ah2mvtfp74       1\n",
      "q7xk33e009       1\n",
      "d8rix1ykp3       1\n",
      "b36nw4lraq       1\n",
      "ub3ssm8b09       1\n",
      "jawie8g9tj       1\n",
      "6kvyu52h3f       1\n",
      "a8kcc3pxp9       1\n",
      "p8ep74z8i4       1\n",
      "9w94dnnpsi       1\n",
      "ov4eqxdx3s       1\n",
      "Name: user_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# frequency of each user_id in sessions data\n",
    "df = sessions['user_id'].value_counts()\n",
    "print (df.shape)\n",
    "print (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updating session_count for users present in the train data\n",
    "\n",
    "train_users['session_count'] = 0\n",
    "\n",
    "for key,val in df.iteritems():\n",
    "    train_users.loc[train_users[ 'id' ] == key, 'session_count'] = val\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644\n"
     ]
    }
   ],
   "source": [
    "print (train_users['session_count'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encding for country_destination\n",
    "country_destination_encoding = {'NDF': 0,\n",
    "'US' : 1,\n",
    "'other' : 2,\n",
    "'FR' : 3,\n",
    "'IT' : 4,\n",
    "'GB' : 5,\n",
    "'ES' : 6,\n",
    "'CA' : 7,\n",
    "'DE' : 8,\n",
    "'NL' : 9,\n",
    "'AU' : 10,\n",
    "'PT' : 11}\n",
    "\n",
    "# Convert series to frame\n",
    "labels_df = train_users_labels.to_frame()\n",
    "\n",
    "for data in [labels_df]:\n",
    "    data['country_destination'] = data['country_destination'].apply(lambda x: country_destination_encoding[x])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn import preprocessing\\n\\nstdscaler = preprocessing.StandardScaler()\\ntrain_users_scaled = stdscaler.fit_transform(train_users.values);\\ntrain_users = pd.DataFrame(train_users_scaled, columns = train_users.columns)\\n\\ntrain_users_merge_scaled = stdscaler.fit_transform(train_users_merge.values);\\ntrain_users_merge = pd.DataFrame(train_users_merge_scaled, columns = train_users_merge.columns)\\n\\n#test_users_scaled = stdscaler.fit_transform(test_users.values);\\n#test_users = pd.DataFrame(test_users_scaled, columns = test_users.columns)\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print train_users_merge.head()\n",
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "\n",
    "stdscaler = preprocessing.StandardScaler()\n",
    "train_users_scaled = stdscaler.fit_transform(train_users.values);\n",
    "train_users = pd.DataFrame(train_users_scaled, columns = train_users.columns)\n",
    "\n",
    "train_users_merge_scaled = stdscaler.fit_transform(train_users_merge.values);\n",
    "train_users_merge = pd.DataFrame(train_users_merge_scaled, columns = train_users_merge.columns)\n",
    "\n",
    "#test_users_scaled = stdscaler.fit_transform(test_users.values);\n",
    "#test_users = pd.DataFrame(test_users_scaled, columns = test_users.columns)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_users['country_destination'] = labels_df\n",
    "#print(train_users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_users_merge['country_destination'] = labels_df\n",
    "#print(train_users_merge.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_users.to_csv('train_users_wo_merge_scale.csv',index=False)\n",
    "#train_users_merge.to_csv('train_users_merge_scale.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_users_merge_wo_scale['country_destination'] = labels_df\n",
    "#train_users_merge_wo_scale.to_csv('train_users_merge_wo_scale.csv',index=False)\n",
    "#test_users_merge_wo_scale.to_csv('test_users_merge_wo_scale.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_user_wo_merge_wo_scale['country_destination'] = labels_df\n",
    "#train_user_wo_merge_wo_scale.to_csv('train_users_wo_merge_wo_scale.csv',index=False)\n",
    "#test_user_wo_merge_wo_scale.to_csv('test_users_wo_merge_wo_scale.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def folds_to_split(data,targets,train,test):\n",
    "    data_tr = pd.DataFrame(data).iloc[train]\n",
    "    data_te = pd.DataFrame(data).iloc[test]\n",
    "    labels_tr = pd.DataFrame(targets).iloc[train]\n",
    "    labels_te = pd.DataFrame(targets).iloc[test]\n",
    "    return [data_tr, data_te, labels_tr, labels_te]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating the NDCG Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_scorer(ndcg_score, needs_proba=True, k=5)\n"
     ]
    }
   ],
   "source": [
    "# Reference Kaggle\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def dcg_score(y_true, y_score, k=5):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "#def ndcg_score(ground_truth, predictions, k=5):\n",
    "def ndcg_score(te_labels, predict, k):\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(len(predict) + 1))\n",
    "    T = lb.transform(te_labels)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predict):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        if best == 0:\n",
    "            best = 0.000000001\n",
    "        score = float(actual) / float(best)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# NDCG Scorer function\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)\n",
    "print ndcg_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print train_users.head()\n",
    "train_users=train_users.drop(['id'], axis=1)\n",
    "#print train_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA using test-train split\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "\n",
    "[tr_data, te_data, tr_labels, te_labels] = cross_validation.train_test_split(train_users, labels_df, test_size=0.33,\n",
    "                                                                             random_state=20160302)\n",
    "lda_clf = LDA()\n",
    "lda_clf.fit(tr_data, tr_labels.values.ravel())\n",
    "ground_truth = te_labels.as_matrix()\n",
    "prob_lda = lda_clf.predict_proba(te_data)\n",
    "score_lda = ndcg_score(ground_truth, prob_lda, k=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score for LDA:\n",
      "0.809272396586\n"
     ]
    }
   ],
   "source": [
    "print \"NDCG Score for LDA:\"\n",
    "print score_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for LDA:\n",
      "0.588565993271\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy Score for LDA:\"\n",
    "print lda_clf.score(te_data,te_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes using test-train split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing, cross_validation\n",
    "\n",
    "gnb = GaussianNB()\n",
    "[tr_data, te_data, \n",
    " tr_labels, te_labels] = cross_validation.train_test_split(train_users, labels_df, test_size=0.33,random_state=20160302)\n",
    "gnb.fit(tr_data, tr_labels.values.ravel())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_arr = gnb.predict_proba(te_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806278567816\n"
     ]
    }
   ],
   "source": [
    "ground_truth = te_labels.as_matrix()\n",
    "predictions = prob_arr\n",
    "score = ndcg_score(ground_truth, predictions, k=5)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score for Naive Bayes:\n",
      "0.806278567816\n"
     ]
    }
   ],
   "source": [
    "print \"NDCG Score for Naive Bayes:\"\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Naive Bayes:\n",
      "0.581013359077\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy Score for Naive Bayes:\"\n",
    "print gnb.score(te_data,te_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score for Adaboost with Naive Bayes:\n",
      "0.807958234839\n"
     ]
    }
   ],
   "source": [
    "# Adaboost with Naive Bayes\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf1 = AdaBoostClassifier(base_estimator=gnb, n_estimators=1000)\n",
    "clf1.fit(tr_data, tr_labels.values.ravel())\n",
    "prob_ada_nb = clf1.predict_proba(te_data)\n",
    "score = ndcg_score(ground_truth, prob_ada_nb, k=5)\n",
    "print \"NDCG Score for Adaboost with Naive Bayes:\"\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Adaboost with Naive Bayes:\n",
      "0.585442723491\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy Score for Adaboost with Naive Bayes:\"\n",
    "print clf1.score(te_data,te_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/anaconda/lib/python2.7/site-packages/sklearn/qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n",
      "/Users/neha/anaconda/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356243751631\n"
     ]
    }
   ],
   "source": [
    "# QDA with test-train split\n",
    "\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "\n",
    "[tr_data, te_data, tr_labels, te_labels] = cross_validation.train_test_split(train_users, labels_df, test_size=0.33,\n",
    "                                                                             random_state=20160302)\n",
    "qda_clf = QDA()\n",
    "qda_clf.fit(tr_data, tr_labels.values.ravel())\n",
    "ground_truth = te_labels.as_matrix()\n",
    "prob_qda = qda_clf.predict_proba(te_data)\n",
    "score_qda = ndcg_score(ground_truth, prob_qda, k=5)\n",
    "print score_qda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score for QDA:\n",
      "0.356243751631\n"
     ]
    }
   ],
   "source": [
    "print \"NDCG Score for QDA:\"\n",
    "print score_qda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for QDA:\n",
      "0.0951745481906\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy Score for QDA:\"\n",
    "print qda_clf.score(te_data,te_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score for Gradient Boosting Classifier:\n",
      "0.824479634631\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier with test-train split, max_depth=3, n_estimators=100\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(max_depth=3, n_estimators=100, random_state=20160302)\n",
    "gb_clf.fit(tr_data, tr_labels.values.ravel())\n",
    "\n",
    "# NDCG of GradientBoostingClassifier\n",
    "prob_gb = gb_clf.predict_proba(te_data)\n",
    "score_gb = ndcg_score(ground_truth, prob_gb, k=5)\n",
    "print \"NDCG Score for Gradient Boosting Classifier:\"\n",
    "print score_gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for Gradient Boosting Classifier:\n",
      "0.630346824912\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy Score for Gradient Boosting Classifier:\"\n",
    "print gb_clf.score(te_data,te_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score 2 for Gradient Boosting Classifier:\n",
      "0.82377984146\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier with max_depth=3, n_estimators=500\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf2 = GradientBoostingClassifier(max_depth=3, n_estimators=500, random_state=20160302)\n",
    "gb_clf2.fit(tr_data, tr_labels.values.ravel())\n",
    "\n",
    "# NDCG of GradientBoostingClassifier\n",
    "prob_gb2 = gb_clf2.predict_proba(te_data)\n",
    "score_gb2 = ndcg_score(ground_truth, prob_gb2, k=5)\n",
    "print \"NDCG Score 2 for Gradient Boosting Classifier:\"\n",
    "print score_gb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score 2 for Gradient Boosting Classifier:\n",
      "0.630517185082\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy Score 2 for Gradient Boosting Classifier:\"\n",
    "print gb_clf2.score(te_data,te_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 10-fold cross validation for analysis\n",
    "\n",
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndcg_LDA    0.807742\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neha/anaconda/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:453: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA with 10-fold cross-validation\n",
    "\n",
    "foldnum = 0\n",
    "fold_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(train_users), n_folds=10, random_state=20160302, shuffle=True):\n",
    "    foldnum+=1\n",
    "    [tr_data, te_data, tr_labels, te_labels] = folds_to_split(train_users,labels_df,train,test)\n",
    "    lda = LDA()\n",
    "    lda.fit(tr_data, tr_labels.values.ravel())\n",
    "    prob_arr_lda = lda.predict_proba(te_data)\n",
    "    #ground_truth = te_labels.as_matrix()\n",
    "    #fold_results.loc[foldnum, 'Accuracy'] = gnb.score(te_data,te_labels)\n",
    "    score_lda = ndcg_score(te_labels.as_matrix(), prob_arr_lda, k=5)\n",
    "    fold_results.loc[foldnum, 'Ndcg_LDA'] = score_lda\n",
    "    \n",
    "print fold_results.mean()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndcg_Gnb    0.805354\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with 10-fold cross-validation\n",
    "foldnum = 0\n",
    "fold_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(train_users), n_folds=10, random_state=20160302, shuffle=True):\n",
    "    foldnum+=1\n",
    "    [tr_data, te_data, tr_labels, te_labels] = folds_to_split(train_users,labels_df,train,test)\n",
    "    gnb1 = GaussianNB()\n",
    "    gnb1.fit(tr_data, tr_labels.values.ravel())\n",
    "    prob_arr_gnb1 = gnb1.predict_proba(te_data)\n",
    "    #ground_truth = te_labels.as_matrix()\n",
    "    #fold_results.loc[foldnum, 'Accuracy'] = gnb.score(te_data,te_labels)\n",
    "    score_gnb1 = ndcg_score(te_labels.as_matrix(), prob_arr_gnb1, k=5)\n",
    "    fold_results.loc[foldnum, 'Ndcg_Gnb'] = score_gnb1\n",
    "    \n",
    "print fold_results.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 10-fold cross-validation produces similar results for LDA and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using scaled data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0 -4.380020 -0.927300 -0.163283 -1.596552 -0.427798 -0.141579 -0.582242   \n",
      "1 -4.357961  1.058047  0.287705 -1.596552 -0.427798 -0.141579  2.556797   \n",
      "2 -4.348661 -0.927300  2.317149  0.628333 -0.035009 -0.141579 -0.582242   \n",
      "3 -4.303076 -0.927300  0.738692 -1.596552 -0.427798 -0.141579 -0.582242   \n",
      "4 -4.283949 -0.927300  0.625945  0.628333 -0.427798 -0.141579 -0.582242   \n",
      "\n",
      "         7         8         9         10        11        12        13  \\\n",
      "0 -0.468760 -0.798954 -0.359375 -0.876174 -0.971889 -3.222044 -0.006939   \n",
      "1  0.251719 -0.798954 -0.359375 -0.876174 -0.971889 -2.156499 -0.315897   \n",
      "2 -0.468760 -0.798954 -0.359375  0.324910  1.091035 -3.222044  0.919936   \n",
      "3 -0.468760 -0.798954 -0.359375 -0.876174  0.059573 -2.156499  1.846811   \n",
      "4 -0.468760 -0.798954 -0.359375 -0.876174 -0.971889 -3.222044  0.919936   \n",
      "\n",
      "         14        15  \n",
      "0  1.387946 -0.345061  \n",
      "1  1.044700 -0.345061  \n",
      "2  1.387946 -0.345061  \n",
      "3 -1.243607 -0.345061  \n",
      "4 -0.213869 -0.345061  \n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "train_users_scaled = pd.DataFrame(preprocessing.StandardScaler().fit_transform(train_users))\n",
    "print train_users_scaled.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA with 10-fold cross-validation using scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndcg_LDA    0.807742\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# LDA with 10-fold cross-validation using scaled data\n",
    "\n",
    "foldnum = 0\n",
    "fold_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(train_users_scaled), n_folds=10, random_state=20160302, shuffle=True):\n",
    "    foldnum+=1\n",
    "    [tr_data, te_data, tr_labels, te_labels] = folds_to_split(train_users_scaled,labels_df,train,test)\n",
    "    lda2 = LDA()\n",
    "    lda2.fit(tr_data, tr_labels.values.ravel())\n",
    "    prob_arr_lda2 = lda2.predict_proba(te_data)\n",
    "    #ground_truth = te_labels.as_matrix()\n",
    "    #fold_results.loc[foldnum, 'Accuracy'] = gnb.score(te_data,te_labels)\n",
    "    score_lda2 = ndcg_score(te_labels.as_matrix(), prob_arr_lda2, k=5)\n",
    "    fold_results.loc[foldnum, 'Ndcg_LDA'] = score_lda2\n",
    "    \n",
    "print fold_results.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes with 10-fold cross-validation using scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndcg_Gnb    0.753768\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with 10-fold cross-validation using scaled data\n",
    "foldnum = 0\n",
    "fold_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(train_users_scaled), n_folds=10, random_state=20160302, shuffle=True):\n",
    "    foldnum+=1\n",
    "    [tr_data, te_data, tr_labels, te_labels] = folds_to_split(train_users_scaled,labels_df,train,test)\n",
    "    gnb2 = GaussianNB()\n",
    "    gnb2.fit(tr_data, tr_labels.values.ravel())\n",
    "    prob_arr_gnb2 = gnb2.predict_proba(te_data)\n",
    "    #ground_truth = te_labels.as_matrix()\n",
    "    #fold_results.loc[foldnum, 'Accuracy'] = gnb.score(te_data,te_labels)\n",
    "    score_gnb2 = ndcg_score(te_labels.as_matrix(), prob_arr_gnb2, k=5)\n",
    "    fold_results.loc[foldnum, 'Ndcg_Gnb'] = score_gnb2\n",
    "    \n",
    "print fold_results.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling does not improve the score for either of the above classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations :\n",
    "\n",
    "Naive Bayes Classification\n",
    "\n",
    "1. Naive Bayes with test-train split : The base accuracy of the Naive Bayes Classifier was found to be 0.581. The probabilities generated by Naive Bayes were used as predictions for the NDCG scorer and the score was 0.806.\n",
    "\n",
    "2. Naive Bayes with 10-fold cross-validation: We used 10-fold cross-validation to split our data into test and train and used the Naive Bayes classifier with it. This gave us an NDCG score of 0.805, which was pretty close to what we got for test-train split.\n",
    "\n",
    "3. Naive Bayes with 10-fold cross-validation using scaled data: We used Standard Scaler to scale the data. It did not prove to be useful. The NDCG score decreased to 0.75.\n",
    "\n",
    "4. We performed AdaBoost multi-class classification with the Naive Bayes classifier as the base estimator. This was repeated for n_estimators = 500 and n_estmators = 1000. The base classifier accuracy was 0.585 and both experiments gave similar results. The NDCG score increased marginally to 0.808.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis\n",
    "\n",
    "1. LDA with test-train split : The base accuracy of the LDA Classifier was found to be 0.59. The probabilities generated by LDA were used as predictions for the NDCG scorer and the score was 0.81.\n",
    "\n",
    "2. LDA with 10-fold cross-validation: We used 10-fold cross-validation to split our data into test and train and used LDA classifier to fit the train data. This gave us an NDCG score of 0.807, which was less than what we got for test-train split.\n",
    "\n",
    "3. LDA with 10-fold cross-validation using scaled data: We used Standard Scaler to scale the data. The NDCG score was 0.807, almost same as what we got with 10-fold cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Discriminant Analysis\n",
    "\n",
    "QDA was performed with a test-train split and 10-fold cross-validation but both these setups did not give us a high NDCG score. The maximum score achieved was 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Classification\n",
    "\n",
    "In our experiments, we set the max_depth to 3 and 5 and set the n_estimators to 100 and 500. Base estimator was set to default init = loss.init_estimator. The accuracy of the base classifier with n_estimators = 100 and max_depth = 3 was 0.63. The NDCG score for this setting came out to be 0.8244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
