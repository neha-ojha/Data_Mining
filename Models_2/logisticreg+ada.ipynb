{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    2\n",
      "4    1\n",
      "Name: country_destination, dtype: int64\n",
      "   timestamp_first_active    gender       age  signup_method  signup_flow  \\\n",
      "0               -4.380020 -0.927300 -0.163283      -1.596552    -0.427798   \n",
      "1               -4.357961  1.058047  0.287705      -1.596552    -0.427798   \n",
      "2               -4.348661 -0.927300  2.317149       0.628333    -0.035009   \n",
      "3               -4.303076 -0.927300  0.738692      -1.596552    -0.427798   \n",
      "4               -4.283949 -0.927300  0.625945       0.628333    -0.427798   \n",
      "\n",
      "   language  affiliate_channel  affiliate_provider  first_affiliate_tracked  \\\n",
      "0 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "1 -0.141579           2.556797            0.251719                -0.798954   \n",
      "2 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "3 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "4 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "\n",
      "   signup_app      ...       Linux Desktop  Mac Desktop  Opera Phone  \\\n",
      "0   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "1   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "2   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "3   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "4   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "\n",
      "     Tablet  Windows Desktop  Windows Phone  iPad Tablet    iPhone  iPodtouch  \\\n",
      "0 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "1 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "2 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "3 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "4 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "\n",
      "   secs_elapsed  \n",
      "0     -0.391996  \n",
      "1     -0.391996  \n",
      "2     -0.391996  \n",
      "3     -0.391996  \n",
      "4     -0.391996  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "Index(['timestamp_first_active', 'gender', 'age', 'signup_method',\n",
      "       'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider',\n",
      "       'first_affiliate_tracked', 'signup_app', 'first_device_type',\n",
      "       'first_browser', 'created_year', 'created_month', 'created_day',\n",
      "       'session_count', '-unknown-_x', 'booking_request', 'booking_response',\n",
      "       'click', 'data', 'message_post', 'modify', 'partner_callback', 'submit',\n",
      "       'view', '-unknown-_y', 'Android App Unknown Phone/Tablet',\n",
      "       'Android Phone', 'Blackberry', 'Chromebook', 'Linux Desktop',\n",
      "       'Mac Desktop', 'Opera Phone', 'Tablet', 'Windows Desktop',\n",
      "       'Windows Phone', 'iPad Tablet', 'iPhone', 'iPodtouch', 'secs_elapsed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the train and the test data \n",
    "#The preprocessing file needs to be run first, it creates the csv file used here\n",
    "train_users = pd.read_csv('train_users_merge_scale.csv')\n",
    "#test_users = pd.read_csv('test_users.csv')\n",
    "\n",
    "\n",
    "# Extracting labels from the train data\n",
    "train_users_labels = train_users.loc[:,'country_destination']\n",
    "print (train_users_labels.head(n=5))\n",
    "\n",
    "# Extracting attributes from the train data\n",
    "train_users_attrs = train_users.iloc[:,:-1]\n",
    "print(train_users_attrs.head(n=5))\n",
    "\n",
    "train_users = train_users_attrs\n",
    "print(train_users.columns)\n",
    "train_users = train_users.drop(['Blackberry'], axis=1)\n",
    "train_users = train_users.drop(['Opera Phone'], axis=1)\n",
    "labels_df = pd.DataFrame(train_users_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def dcg_score(y_true, y_score, k=5):\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "#def ndcg_score(ground_truth, predictions, k=5):\n",
    "def ndcg_score(te_labels, predict, k):\n",
    "   \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(12 + 1))\n",
    "    T = lb.transform(te_labels)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predict):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        if best == 0:\n",
    "            best = 0.000000001\n",
    "        score = float(actual) / float(best)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# NDCG Scorer function\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def folds_to_split(data,targets,train,test):\n",
    "    data_tr = pd.DataFrame(data).iloc[train]\n",
    "    data_te = pd.DataFrame(data).iloc[test]\n",
    "    labels_tr = pd.DataFrame(targets).iloc[train]\n",
    "    labels_te = pd.DataFrame(targets).iloc[test]\n",
    "    return [data_tr, data_te, labels_tr, labels_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy_newton-cg  ndcg_newton-cg  Accuracy_lbfgs  ndcg_lbfgs  \\\n",
      "1             0.607278        0.815000        0.607278    0.815000   \n",
      "2             0.603587        0.813433        0.603587    0.813436   \n",
      "3             0.604344        0.812489        0.604344    0.812489   \n",
      "4             0.602073        0.812163        0.602073    0.812145   \n",
      "5             0.604581        0.813422        0.604581    0.813422   \n",
      "6             0.613099        0.817676        0.613099    0.817676   \n",
      "7             0.610212        0.816933        0.610212    0.816914   \n",
      "8             0.610117        0.816962        0.610117    0.816962   \n",
      "9             0.608272        0.815800        0.608272    0.815800   \n",
      "10            0.608461        0.816193        0.608461    0.816191   \n",
      "\n",
      "    Accuracy_liblinear  ndcg_liblinear  \n",
      "1             0.607278        0.815000  \n",
      "2             0.603587        0.813433  \n",
      "3             0.604344        0.812489  \n",
      "4             0.602073        0.812163  \n",
      "5             0.604581        0.813422  \n",
      "6             0.613099        0.817676  \n",
      "7             0.610212        0.816933  \n",
      "8             0.610165        0.816979  \n",
      "9             0.608272        0.815819  \n",
      "10            0.608461        0.816193  \n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy_newton-cg    0.607202\n",
      "ndcg_newton-cg        0.815007\n",
      "Accuracy_lbfgs        0.607202\n",
      "ndcg_lbfgs            0.815004\n",
      "Accuracy_liblinear    0.607207\n",
      "ndcg_liblinear        0.815011\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression : One Versus Rest\n",
    "# Use validation set to find which solver to use \n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model, cross_validation\n",
    "\n",
    "fold_results_ovr = pd.DataFrame()\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "    \n",
    "def ten_fold_oneVsRest(data, labels):\n",
    "    \n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True, random_state=20160202):\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        \n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "        \n",
    "    foldnum = 0\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "        train_sets[x], \n",
    "        train_set_labels[x], \n",
    "        test_size=0.11, \n",
    "        random_state=20160121)\n",
    "        \n",
    "        for sol in solvers:\n",
    "            oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver=sol))\n",
    "            #print(tr_target)\n",
    "            oneVsRest.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "            #print(oneVsRest.estimators_)\n",
    "        \n",
    "            fold_results_ovr.loc[foldnum, 'Accuracy_' + sol] = oneVsRest.score(bnb_validation, bnb_validation_labels)\n",
    "        \n",
    "            predictions = oneVsRest.predict_proba(bnb_validation)\n",
    "            score = ndcg_score(bnb_validation_labels.as_matrix(), predictions, 5)\n",
    "            fold_results_ovr.loc[foldnum, 'ndcg_' + sol]  = score\n",
    "            #print(score)\n",
    "                \n",
    "    #Now let's look at the results:\n",
    "    print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "    print (fold_results_ovr)\n",
    "    print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "    print(fold_results_ovr.mean())\n",
    "    \n",
    "ten_fold_oneVsRest(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.608076  0.816181\n",
      "2   0.609885  0.816683\n",
      "3   0.604685  0.813442\n",
      "4   0.606840  0.815432\n",
      "5   0.606606  0.815835\n",
      "6   0.604872  0.814452\n",
      "7   0.607121  0.813901\n",
      "8   0.602670  0.813187\n",
      "9   0.610588  0.818077\n",
      "10  0.611525  0.816938\n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy    0.607287\n",
      "ndcg        0.815413\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fold_results_ovr = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    foldnum+=1\n",
    "    oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver='lbfgs', penalty='l2'))\n",
    "    oneVsRest.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovr.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "    predictions = oneVsRest.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_ovr.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "print (fold_results_ovr)\n",
    "print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "print(fold_results_ovr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.608076  0.816163\n",
      "2   0.609885  0.816683\n",
      "3   0.604685  0.813442\n",
      "4   0.606840  0.815413\n",
      "5   0.606606  0.815870\n",
      "6   0.604872  0.814452\n",
      "7   0.607121  0.813901\n",
      "8   0.602670  0.813169\n",
      "9   0.610588  0.818077\n",
      "10  0.611525  0.816955\n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy    0.607287\n",
      "ndcg        0.815412\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# liblinear and lbfgs perform  nearly the same...\n",
    "\n",
    "fold_results_ovr = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    foldnum+=1\n",
    "    oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver='liblinear', penalty='l2'))\n",
    "    oneVsRest.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovr.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "    predictions = oneVsRest.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_ovr.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "print (fold_results_ovr)\n",
    "print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "print(fold_results_ovr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------OneVsRestClassifier accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.608076  0.816199\n",
      "2   0.609885  0.816708\n",
      "3   0.604685  0.813442\n",
      "4   0.606840  0.815416\n",
      "5   0.606606  0.815858\n",
      "6   0.604872  0.814483\n",
      "7   0.607121  0.813844\n",
      "8   0.602670  0.813170\n",
      "9   0.610588  0.818057\n",
      "10  0.611525  0.816993\n",
      "------OneVsRestClassifier mean accuracy, ndcg values------\n",
      "Accuracy    0.607287\n",
      "ndcg        0.815417\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# liblinear does better with l1 penalty...\n",
    "\n",
    "fold_results_ovr = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    foldnum+=1\n",
    "    oneVsRest = OneVsRestClassifier(linear_model.LogisticRegression(solver='liblinear', penalty='l1'))\n",
    "    oneVsRest.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovr.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "    predictions = oneVsRest.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_ovr.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "print (\"------OneVsRestClassifier accuracy and ndcg values------\")\n",
    "print (fold_results_ovr)\n",
    "print (\"------OneVsRestClassifier mean accuracy, ndcg values------\")\n",
    "print(fold_results_ovr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------OneVsOneClassifier results for accuracy-----------\n",
      "    Accuracy_newton-cg  Accuracy_lbfgs  Accuracy_liblinear\n",
      "1             0.604770        0.604770            0.604770\n",
      "2             0.609218        0.609218            0.609218\n",
      "3             0.608556        0.608556            0.608556\n",
      "4             0.605622        0.605622            0.605622\n",
      "5             0.612152        0.612152            0.612152\n",
      "6             0.605338        0.605338            0.605338\n",
      "7             0.602499        0.602499            0.602499\n",
      "8             0.610969        0.611016            0.610969\n",
      "9             0.608840        0.608840            0.608840\n",
      "10            0.605858        0.605858            0.605858\n",
      "--------------OneVsOneClassifier mean accuracy----------------\n",
      "Accuracy_newton-cg    0.607382\n",
      "Accuracy_lbfgs        0.607387\n",
      "Accuracy_liblinear    0.607382\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanjana\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\sanjana\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:193: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression : One Versus One\n",
    "# Use validation set to find which solver to use \n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "fold_results_ovo = pd.DataFrame()\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "\n",
    "def ten_fold_oneVsOne(data, labels):\n",
    "    foldnum = 0\n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    \n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True):\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        \n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "    \n",
    "    foldnum = 0\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "        train_sets[x], \n",
    "        train_set_labels[x], \n",
    "        test_size=0.11, \n",
    "        random_state=20160121)\n",
    "        for sol in solvers:\n",
    "            oneVsOne = OneVsOneClassifier(linear_model.LogisticRegression(solver=sol))\n",
    "            oneVsOne.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "        \n",
    "            columnname = \"Accuracy\" + \"_\" + sol\n",
    "            fold_results_ovo.loc[foldnum, columnname] = oneVsOne.score(bnb_validation, bnb_validation_labels)\n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"-----------OneVsOneClassifier results for accuracy-----------\")\n",
    "    print (fold_results_ovo)\n",
    "    \n",
    "    print (\"--------------OneVsOneClassifier mean accuracy----------------\")\n",
    "    print(fold_results_ovo.mean())\n",
    "    \n",
    "ten_fold_oneVsOne(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------OneVsOneClassifier results for accuracy-----------\n",
      "    Accuracy\n",
      "1   0.602080\n",
      "2   0.607309\n",
      "3   0.608855\n",
      "4   0.607871\n",
      "5   0.605247\n",
      "6   0.600609\n",
      "7   0.608199\n",
      "8   0.612087\n",
      "9   0.609932\n",
      "10  0.610354\n",
      "--------------OneVsOneClassifier mean accuracy----------------\n",
      "Accuracy    0.607254\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# lbfgs does best every time...\n",
    "\n",
    "fold_results_ovo = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    \n",
    "    foldnum+=1\n",
    "    oneVsOne = OneVsOneClassifier(linear_model.LogisticRegression(solver='lbfgs'))\n",
    "    oneVsOne.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ovo.loc[foldnum, 'Accuracy'] = oneVsOne.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "\n",
    "print (\"-----------OneVsOneClassifier results for accuracy-----------\")\n",
    "print (fold_results_ovo)\n",
    "print (\"--------------OneVsOneClassifier mean accuracy----------------\")\n",
    "print(fold_results_ovo.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806944035724: solver newton-cg, estimators 100\n",
      "0.806926570731: solver newton-cg, estimators 200\n",
      "0.806926570731: solver newton-cg, estimators 250\n",
      "0.806926570731: solver lbfgs, estimators 100\n",
      "0.806909105737: solver lbfgs, estimators 200\n",
      "0.806926570731: solver lbfgs, estimators 250\n",
      "0.8052099633: solver newton-cg, estimators 100\n",
      "0.8052099633: solver newton-cg, estimators 200\n",
      "0.8052099633: solver newton-cg, estimators 250\n",
      "0.8052099633: solver lbfgs, estimators 100\n",
      "0.805244893287: solver lbfgs, estimators 200\n",
      "0.805244893287: solver lbfgs, estimators 250\n",
      "0.803373399797: solver newton-cg, estimators 100\n",
      "0.80339086479: solver newton-cg, estimators 200\n",
      "0.80339086479: solver newton-cg, estimators 250\n",
      "0.80339086479: solver lbfgs, estimators 100\n",
      "0.803408329784: solver lbfgs, estimators 200\n",
      "0.803408329784: solver lbfgs, estimators 250\n",
      "0.804220364724: solver newton-cg, estimators 100\n",
      "0.804237829718: solver newton-cg, estimators 200\n",
      "0.804237829718: solver newton-cg, estimators 250\n",
      "0.804150504749: solver lbfgs, estimators 100\n",
      "0.80420289973: solver lbfgs, estimators 200\n",
      "0.80420289973: solver lbfgs, estimators 250\n",
      "0.806519123405: solver newton-cg, estimators 100\n",
      "0.806519123405: solver newton-cg, estimators 200\n",
      "0.806519123405: solver newton-cg, estimators 250\n",
      "0.806536588399: solver lbfgs, estimators 100\n",
      "0.806536588399: solver lbfgs, estimators 200\n",
      "0.806554053393: solver lbfgs, estimators 250\n",
      "0.809865522653: solver newton-cg, estimators 100\n",
      "0.809865522653: solver newton-cg, estimators 200\n",
      "0.809882987647: solver newton-cg, estimators 250\n",
      "0.809848057659: solver lbfgs, estimators 100\n",
      "0.809865522653: solver lbfgs, estimators 200\n",
      "0.809865522653: solver lbfgs, estimators 250\n",
      "0.809451482845: solver newton-cg, estimators 100\n",
      "0.809468947838: solver newton-cg, estimators 200\n",
      "0.809468947838: solver newton-cg, estimators 250\n",
      "0.809329227889: solver lbfgs, estimators 100\n",
      "0.809294297901: solver lbfgs, estimators 200\n",
      "0.809311762895: solver lbfgs, estimators 250\n",
      "0.808491978033: solver newton-cg, estimators 100\n",
      "0.808509443026: solver newton-cg, estimators 200\n",
      "0.808509443026: solver newton-cg, estimators 250\n",
      "0.80838718807: solver lbfgs, estimators 100\n",
      "0.808439583052: solver lbfgs, estimators 200\n",
      "0.808439583052: solver lbfgs, estimators 250\n",
      "0.808080791909: solver newton-cg, estimators 100\n",
      "0.808080791909: solver newton-cg, estimators 200\n",
      "0.808080791909: solver newton-cg, estimators 250\n",
      "0.808220511858: solver lbfgs, estimators 100\n",
      "0.808220511858: solver lbfgs, estimators 200\n",
      "0.808220511858: solver lbfgs, estimators 250\n",
      "0.807305636316: solver newton-cg, estimators 100\n",
      "0.807358031297: solver newton-cg, estimators 200\n",
      "0.807358031297: solver newton-cg, estimators 250\n",
      "0.807218311347: solver lbfgs, estimators 100\n",
      "0.807235776341: solver lbfgs, estimators 200\n",
      "0.807235776341: solver lbfgs, estimators 250\n",
      "------Ada accuracy and ndcg values------\n",
      "    Accuracy_newton-cg_100  ndcg_newton-cg_100  Accuracy_newton-cg_200  \\\n",
      "1                 0.604013            0.806944                0.604297   \n",
      "2                 0.594974            0.805210                0.599849   \n",
      "3                 0.598240            0.803373                0.599896   \n",
      "4                 0.600274            0.804220                0.601599   \n",
      "5                 0.598949            0.806519                0.602309   \n",
      "6                 0.608982            0.809866                0.608934   \n",
      "7                 0.599754            0.809451                0.600984   \n",
      "8                 0.603350            0.808492                0.603634   \n",
      "9                 0.599091            0.808081                0.599896   \n",
      "10                0.602120            0.807306                0.603256   \n",
      "\n",
      "    ndcg_newton-cg_200  Accuracy_newton-cg_250  ndcg_newton-cg_250  \\\n",
      "1             0.806927                0.604581            0.806927   \n",
      "2             0.805210                0.600274            0.805210   \n",
      "3             0.803391                0.600606            0.803391   \n",
      "4             0.804238                0.601694            0.804238   \n",
      "5             0.806519                0.603019            0.806519   \n",
      "6             0.809866                0.609124            0.809883   \n",
      "7             0.809469                0.599991            0.809469   \n",
      "8             0.808509                0.604533            0.808509   \n",
      "9             0.808081                0.601079            0.808081   \n",
      "10            0.807358                0.602499            0.807358   \n",
      "\n",
      "    Accuracy_lbfgs_100  ndcg_lbfgs_100  Accuracy_lbfgs_200  ndcg_lbfgs_200  \\\n",
      "1             0.601647        0.806927            0.601647        0.806909   \n",
      "2             0.599044        0.805210            0.600322        0.805245   \n",
      "3             0.601931        0.803391            0.601789        0.803408   \n",
      "4             0.598997        0.804151            0.600133        0.804203   \n",
      "5             0.600416        0.806537            0.602404        0.806537   \n",
      "6             0.608650        0.809848            0.609360        0.809866   \n",
      "7             0.605007        0.809329            0.606048        0.809294   \n",
      "8             0.600180        0.808387            0.602593        0.808440   \n",
      "9             0.606048        0.808221            0.605480        0.808221   \n",
      "10            0.606379        0.807218            0.607420        0.807236   \n",
      "\n",
      "    Accuracy_lbfgs_250  ndcg_lbfgs_250  \n",
      "1             0.602451        0.806927  \n",
      "2             0.600085        0.805245  \n",
      "3             0.602025        0.803408  \n",
      "4             0.600416        0.804203  \n",
      "5             0.602593        0.806554  \n",
      "6             0.609124        0.809866  \n",
      "7             0.605906        0.809312  \n",
      "8             0.602167        0.808440  \n",
      "9             0.605291        0.808221  \n",
      "10            0.607089        0.807236  \n",
      "------Ada mean accuracy and ndcg values------\n",
      "Accuracy_newton-cg_100    0.600975\n",
      "ndcg_newton-cg_100        0.806946\n",
      "Accuracy_newton-cg_200    0.602465\n",
      "ndcg_newton-cg_200        0.806957\n",
      "Accuracy_newton-cg_250    0.602740\n",
      "ndcg_newton-cg_250        0.806958\n",
      "Accuracy_lbfgs_100        0.602830\n",
      "ndcg_lbfgs_100            0.806922\n",
      "Accuracy_lbfgs_200        0.603719\n",
      "ndcg_lbfgs_200            0.806936\n",
      "Accuracy_lbfgs_250        0.603715\n",
      "ndcg_lbfgs_250            0.806941\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use ADA boost with logistsic regression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model, cross_validation\n",
    "\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "fold_results_ada = pd.DataFrame()\n",
    "def ten_fold_ada_logistic(data, labels):\n",
    "    foldnum = 0\n",
    "    fold_results_ovr_est = pd.DataFrame()\n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True, random_state=20160202):\n",
    "        \n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "    \n",
    "    solvers = ['newton-cg', 'lbfgs']\n",
    "    estimators = [100, 200, 250]\n",
    "    foldnum = 0\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "            train_sets[x], \n",
    "            train_set_labels[x], \n",
    "            test_size=0.11, \n",
    "            random_state=20160121)\n",
    "        for sol in solvers:\n",
    "            for est in estimators:\n",
    "                \n",
    "                [tr_data, te_data,\n",
    "                 tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "                ada = AdaBoostClassifier(learning_rate=0.3, n_estimators=est, \n",
    "                                     base_estimator=linear_model.LogisticRegression(solver=sol), \n",
    "                                     algorithm='SAMME')  \n",
    "                #print(tr_target)\n",
    "                ada.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "                #print(oneVsRest.estimators_)\n",
    "        \n",
    "                fold_results_ada.loc[foldnum, 'Accuracy_' + sol + '_' + str(est)] = ada.score(bnb_validation, \n",
    "                                                                                            bnb_validation_labels)\n",
    "        \n",
    "                predictions = ada.predict_proba(bnb_validation)\n",
    "                score = ndcg_score(bnb_validation_labels.as_matrix(), predictions, 5)\n",
    "                fold_results_ada.loc[foldnum, 'ndcg_' + sol + '_' + str(est)]  = score\n",
    "                print(str(score) + \":\" + \" solver \" + sol + \", estimators \" + str(est))\n",
    "            \n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"------Ada accuracy and ndcg values------\")\n",
    "    print (fold_results_ada)\n",
    "    print (\"------Ada mean accuracy and ndcg values------\")\n",
    "    print(fold_results_ada.mean())\n",
    "    \n",
    "ten_fold_ada_logistic(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Ada accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.598426  0.808153\n",
      "2   0.604825  0.809260\n",
      "3   0.600141  0.805607\n",
      "4   0.601265  0.807822\n",
      "5   0.604170  0.807697\n",
      "6   0.598688  0.806826\n",
      "7   0.605107  0.805142\n",
      "8   0.593769  0.804849\n",
      "9   0.604029  0.810024\n",
      "10  0.604966  0.809372\n",
      "------Ada mean accuracy and ndcg values------\n",
      "Accuracy    0.601539\n",
      "ndcg        0.807475\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# The best value is for newton-cg, estimators: 250\n",
    "\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    # one fourth of the training data is for validation\n",
    "    foldnum+=1\n",
    "    ada = AdaBoostClassifier(learning_rate=0.3, n_estimators=250, \n",
    "                                     base_estimator=linear_model.LogisticRegression(solver='newton-cg'), \n",
    "                                     algorithm='SAMME') \n",
    "    \n",
    "    ada.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_ada.loc[foldnum, 'Accuracy'] = ada.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "        \n",
    "    predictions = ada.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_ada.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "\n",
    "print (\"------Ada accuracy and ndcg values------\")\n",
    "print (fold_results_ada)\n",
    "print (\"------Ada mean accuracy and ndcg values------\")\n",
    "print(fold_results_ada.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_penl1_numIter20: 0.409331819042\n",
      "Accuracy_penl2_numIter20: 0.389977285633\n",
      "Accuracy_penNone_numIter20: 0.428165814878\n",
      "Accuracy_penl1_numIter50: 0.307117168276\n",
      "Accuracy_penl2_numIter50: 0.453340904789\n",
      "Accuracy_penNone_numIter50: 0.396886238879\n",
      "Accuracy_penl1_numIter100: 0.529765284876\n",
      "Accuracy_penl2_numIter100: 0.523566155593\n",
      "Accuracy_penNone_numIter100: 0.515710770396\n",
      "Accuracy_penl1_numIter200: 0.451779292069\n",
      "Accuracy_penl2_numIter200: 0.442551580541\n",
      "Accuracy_penNone_numIter200: 0.437488169601\n",
      "Accuracy_penl1_numIter500: 0.599375354912\n",
      "Accuracy_penl2_numIter500: 0.536816202915\n",
      "Accuracy_penNone_numIter500: 0.557259133068\n",
      "Accuracy_penl1_numIter20: 0.526310808253\n",
      "Accuracy_penl2_numIter20: 0.558820745788\n",
      "Accuracy_penNone_numIter20: 0.53653227333\n",
      "Accuracy_penl1_numIter50: 0.474399015711\n",
      "Accuracy_penl2_numIter50: 0.502082150293\n",
      "Accuracy_penNone_numIter50: 0.485377626349\n",
      "Accuracy_penl1_numIter100: 0.561612720045\n",
      "Accuracy_penl2_numIter100: 0.523660798789\n",
      "Accuracy_penNone_numIter100: 0.51230361537\n",
      "Accuracy_penl1_numIter200: 0.494179443498\n",
      "Accuracy_penl2_numIter200: 0.522430437252\n",
      "Accuracy_penNone_numIter200: 0.544387658527\n",
      "Accuracy_penl1_numIter500: 0.203814120765\n",
      "Accuracy_penl2_numIter500: 0.509700927503\n",
      "Accuracy_penNone_numIter500: 0.460439144426\n",
      "Accuracy_penl1_numIter20: 0.374597766421\n",
      "Accuracy_penl2_numIter20: 0.452631080825\n",
      "Accuracy_penNone_numIter20: 0.433134582624\n",
      "Accuracy_penl1_numIter50: 0.280427787242\n",
      "Accuracy_penl2_numIter50: 0.415199697142\n",
      "Accuracy_penNone_numIter50: 0.436305129661\n",
      "Accuracy_penl1_numIter100: 0.561470755253\n",
      "Accuracy_penl2_numIter100: 0.497539276926\n",
      "Accuracy_penNone_numIter100: 0.472364187015\n",
      "Accuracy_penl1_numIter200: 0.441936399773\n",
      "Accuracy_penl2_numIter200: 0.457978421352\n",
      "Accuracy_penNone_numIter200: 0.48144993375\n",
      "Accuracy_penl1_numIter500: 0.378762067007\n",
      "Accuracy_penl2_numIter500: 0.387469240962\n",
      "Accuracy_penNone_numIter500: 0.431809577891\n",
      "Accuracy_penl1_numIter20: 0.412029150104\n",
      "Accuracy_penl2_numIter20: 0.473026689381\n",
      "Accuracy_penNone_numIter20: 0.518266136665\n",
      "Accuracy_penl1_numIter50: 0.580352072686\n",
      "Accuracy_penl2_numIter50: 0.434932803331\n",
      "Accuracy_penNone_numIter50: 0.469808820746\n",
      "Accuracy_penl1_numIter100: 0.375402233579\n",
      "Accuracy_penl2_numIter100: 0.45078553852\n",
      "Accuracy_penNone_numIter100: 0.464840053\n",
      "Accuracy_penl1_numIter200: 0.409899678213\n",
      "Accuracy_penl2_numIter200: 0.447614991482\n",
      "Accuracy_penNone_numIter200: 0.497539276926\n",
      "Accuracy_penl1_numIter500: 0.374787052811\n",
      "Accuracy_penl2_numIter500: 0.490299072497\n",
      "Accuracy_penNone_numIter500: 0.489873178118\n",
      "Accuracy_penl1_numIter20: 0.295050160893\n",
      "Accuracy_penl2_numIter20: 0.452820367216\n",
      "Accuracy_penNone_numIter20: 0.539939428355\n",
      "Accuracy_penl1_numIter50: 0.47539276926\n",
      "Accuracy_penl2_numIter50: 0.514433087261\n",
      "Accuracy_penNone_numIter50: 0.495646413023\n",
      "Accuracy_penl1_numIter100: 0.525600984289\n",
      "Accuracy_penl2_numIter100: 0.478137421919\n",
      "Accuracy_penNone_numIter100: 0.450548930532\n",
      "Accuracy_penl1_numIter200: 0.284544766231\n",
      "Accuracy_penl2_numIter200: 0.486229415105\n",
      "Accuracy_penNone_numIter200: 0.461574862767\n",
      "Accuracy_penl1_numIter500: 0.571644898732\n",
      "Accuracy_penl2_numIter500: 0.395229982964\n",
      "Accuracy_penNone_numIter500: 0.443781942078\n",
      "Accuracy_penl1_numIter20: 0.417329169033\n",
      "Accuracy_penl2_numIter20: 0.48594548552\n",
      "Accuracy_penNone_numIter20: 0.475818663638\n",
      "Accuracy_penl1_numIter50: 0.437204240015\n",
      "Accuracy_penl2_numIter50: 0.469808820746\n",
      "Accuracy_penNone_numIter50: 0.461290933182\n",
      "Accuracy_penl1_numIter100: 0.129187961386\n",
      "Accuracy_penl2_numIter100: 0.487459776642\n",
      "Accuracy_penNone_numIter100: 0.482964224872\n",
      "Accuracy_penl1_numIter200: 0.456369487034\n",
      "Accuracy_penl2_numIter200: 0.510978610638\n",
      "Accuracy_penNone_numIter200: 0.472695438198\n",
      "Accuracy_penl1_numIter500: 0.50449555177\n",
      "Accuracy_penl2_numIter500: 0.496829452962\n",
      "Accuracy_penNone_numIter500: 0.530617073632\n",
      "Accuracy_penl1_numIter20: 0.30546091236\n",
      "Accuracy_penl2_numIter20: 0.468767745599\n",
      "Accuracy_penNone_numIter20: 0.469761499148\n",
      "Accuracy_penl1_numIter50: 0.576519023282\n",
      "Accuracy_penl2_numIter50: 0.551627862957\n",
      "Accuracy_penNone_numIter50: 0.51973310619\n",
      "Accuracy_penl1_numIter100: 0.503785727806\n",
      "Accuracy_penl2_numIter100: 0.452489116033\n",
      "Accuracy_penNone_numIter100: 0.434128336173\n",
      "Accuracy_penl1_numIter200: 0.422581866364\n",
      "Accuracy_penl2_numIter200: 0.499526784024\n",
      "Accuracy_penNone_numIter200: 0.5566439523\n",
      "Accuracy_penl1_numIter500: 0.43848192315\n",
      "Accuracy_penl2_numIter500: 0.503927692599\n",
      "Accuracy_penNone_numIter500: 0.5136759417\n",
      "Accuracy_penl1_numIter20: 0.427172061329\n",
      "Accuracy_penl2_numIter20: 0.402943403369\n",
      "Accuracy_penNone_numIter20: 0.454807874314\n",
      "Accuracy_penl1_numIter50: 0.264101836078\n",
      "Accuracy_penl2_numIter50: 0.386617452205\n",
      "Accuracy_penNone_numIter50: 0.400530001893\n",
      "Accuracy_penl1_numIter100: 0.476481166004\n",
      "Accuracy_penl2_numIter100: 0.495031232254\n",
      "Accuracy_penNone_numIter100: 0.524465265947\n",
      "Accuracy_penl1_numIter200: 0.589721749006\n",
      "Accuracy_penl2_numIter200: 0.529623320083\n",
      "Accuracy_penNone_numIter200: 0.53828317244\n",
      "Accuracy_penl1_numIter500: 0.436352451259\n",
      "Accuracy_penl2_numIter500: 0.473310618966\n",
      "Accuracy_penNone_numIter500: 0.486607987886\n",
      "Accuracy_penl1_numIter20: 0.417139882642\n",
      "Accuracy_penl2_numIter20: 0.469240961575\n",
      "Accuracy_penNone_numIter20: 0.436731024039\n",
      "Accuracy_penl1_numIter50: 0.387989778535\n",
      "Accuracy_penl2_numIter50: 0.525080446716\n",
      "Accuracy_penNone_numIter50: 0.519685784592\n",
      "Accuracy_penl1_numIter100: 0.33749763392\n",
      "Accuracy_penl2_numIter100: 0.479509748249\n",
      "Accuracy_penNone_numIter100: 0.468199886428\n",
      "Accuracy_penl1_numIter200: 0.431005110733\n",
      "Accuracy_penl2_numIter200: 0.469430247965\n",
      "Accuracy_penNone_numIter200: 0.47402044293\n",
      "Accuracy_penl1_numIter500: 0.487270490252\n",
      "Accuracy_penl2_numIter500: 0.434980124929\n",
      "Accuracy_penNone_numIter500: 0.354675373841\n",
      "Accuracy_penl1_numIter20: 0.583759227712\n",
      "Accuracy_penl2_numIter20: 0.520395608556\n",
      "Accuracy_penNone_numIter20: 0.559530569752\n",
      "Accuracy_penl1_numIter50: 0.398779102783\n",
      "Accuracy_penl2_numIter50: 0.454713231119\n",
      "Accuracy_penNone_numIter50: 0.473405262162\n",
      "Accuracy_penl1_numIter100: 0.393005867878\n",
      "Accuracy_penl2_numIter100: 0.435973878478\n",
      "Accuracy_penNone_numIter100: 0.457741813364\n",
      "Accuracy_penl1_numIter200: 0.468247208026\n",
      "Accuracy_penl2_numIter200: 0.553426083665\n",
      "Accuracy_penNone_numIter200: 0.537336740488\n",
      "Accuracy_penl1_numIter500: 0.554561802006\n",
      "Accuracy_penl2_numIter500: 0.543772477759\n",
      "Accuracy_penNone_numIter500: 0.538425137233\n",
      "-----------OneVsRestClassifier Perceptron results for accuracy-----------\n",
      "    Accuracy_penl1_numIter20  Accuracy_penl2_numIter20  \\\n",
      "1                   0.409332                  0.389977   \n",
      "2                   0.526311                  0.558821   \n",
      "3                   0.374598                  0.452631   \n",
      "4                   0.412029                  0.473027   \n",
      "5                   0.295050                  0.452820   \n",
      "6                   0.417329                  0.485945   \n",
      "7                   0.305461                  0.468768   \n",
      "8                   0.427172                  0.402943   \n",
      "9                   0.417140                  0.469241   \n",
      "10                  0.583759                  0.520396   \n",
      "\n",
      "    Accuracy_penNone_numIter20  Accuracy_penl1_numIter50  \\\n",
      "1                     0.428166                  0.307117   \n",
      "2                     0.536532                  0.474399   \n",
      "3                     0.433135                  0.280428   \n",
      "4                     0.518266                  0.580352   \n",
      "5                     0.539939                  0.475393   \n",
      "6                     0.475819                  0.437204   \n",
      "7                     0.469761                  0.576519   \n",
      "8                     0.454808                  0.264102   \n",
      "9                     0.436731                  0.387990   \n",
      "10                    0.559531                  0.398779   \n",
      "\n",
      "    Accuracy_penl2_numIter50  Accuracy_penNone_numIter50  \\\n",
      "1                   0.453341                    0.396886   \n",
      "2                   0.502082                    0.485378   \n",
      "3                   0.415200                    0.436305   \n",
      "4                   0.434933                    0.469809   \n",
      "5                   0.514433                    0.495646   \n",
      "6                   0.469809                    0.461291   \n",
      "7                   0.551628                    0.519733   \n",
      "8                   0.386617                    0.400530   \n",
      "9                   0.525080                    0.519686   \n",
      "10                  0.454713                    0.473405   \n",
      "\n",
      "    Accuracy_penl1_numIter100  Accuracy_penl2_numIter100  \\\n",
      "1                    0.529765                   0.523566   \n",
      "2                    0.561613                   0.523661   \n",
      "3                    0.561471                   0.497539   \n",
      "4                    0.375402                   0.450786   \n",
      "5                    0.525601                   0.478137   \n",
      "6                    0.129188                   0.487460   \n",
      "7                    0.503786                   0.452489   \n",
      "8                    0.476481                   0.495031   \n",
      "9                    0.337498                   0.479510   \n",
      "10                   0.393006                   0.435974   \n",
      "\n",
      "    Accuracy_penNone_numIter100  Accuracy_penl1_numIter200  \\\n",
      "1                      0.515711                   0.451779   \n",
      "2                      0.512304                   0.494179   \n",
      "3                      0.472364                   0.441936   \n",
      "4                      0.464840                   0.409900   \n",
      "5                      0.450549                   0.284545   \n",
      "6                      0.482964                   0.456369   \n",
      "7                      0.434128                   0.422582   \n",
      "8                      0.524465                   0.589722   \n",
      "9                      0.468200                   0.431005   \n",
      "10                     0.457742                   0.468247   \n",
      "\n",
      "    Accuracy_penl2_numIter200  Accuracy_penNone_numIter200  \\\n",
      "1                    0.442552                     0.437488   \n",
      "2                    0.522430                     0.544388   \n",
      "3                    0.457978                     0.481450   \n",
      "4                    0.447615                     0.497539   \n",
      "5                    0.486229                     0.461575   \n",
      "6                    0.510979                     0.472695   \n",
      "7                    0.499527                     0.556644   \n",
      "8                    0.529623                     0.538283   \n",
      "9                    0.469430                     0.474020   \n",
      "10                   0.553426                     0.537337   \n",
      "\n",
      "    Accuracy_penl1_numIter500  Accuracy_penl2_numIter500  \\\n",
      "1                    0.599375                   0.536816   \n",
      "2                    0.203814                   0.509701   \n",
      "3                    0.378762                   0.387469   \n",
      "4                    0.374787                   0.490299   \n",
      "5                    0.571645                   0.395230   \n",
      "6                    0.504496                   0.496829   \n",
      "7                    0.438482                   0.503928   \n",
      "8                    0.436352                   0.473311   \n",
      "9                    0.487270                   0.434980   \n",
      "10                   0.554562                   0.543772   \n",
      "\n",
      "    Accuracy_penNone_numIter500  \n",
      "1                      0.557259  \n",
      "2                      0.460439  \n",
      "3                      0.431810  \n",
      "4                      0.489873  \n",
      "5                      0.443782  \n",
      "6                      0.530617  \n",
      "7                      0.513676  \n",
      "8                      0.486608  \n",
      "9                      0.354675  \n",
      "10                     0.538425  \n",
      "--------------OneVsRestClassifier Perceptron mean accuracy----------------\n",
      "Accuracy_penl1_numIter20       0.416818\n",
      "Accuracy_penl2_numIter20       0.467457\n",
      "Accuracy_penNone_numIter20     0.485269\n",
      "Accuracy_penl1_numIter50       0.418228\n",
      "Accuracy_penl2_numIter50       0.470784\n",
      "Accuracy_penNone_numIter50     0.465867\n",
      "Accuracy_penl1_numIter100      0.439381\n",
      "Accuracy_penl2_numIter100      0.482415\n",
      "Accuracy_penNone_numIter100    0.478327\n",
      "Accuracy_penl1_numIter200      0.445027\n",
      "Accuracy_penl2_numIter200      0.491979\n",
      "Accuracy_penNone_numIter200    0.500142\n",
      "Accuracy_penl1_numIter500      0.454955\n",
      "Accuracy_penl2_numIter500      0.477234\n",
      "Accuracy_penNone_numIter500    0.480716\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "fold_results_perc = pd.DataFrame()\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "\n",
    "def ten_fold_oneVsRest_Perc(data, labels):\n",
    "    foldnum = 0\n",
    "    iters = [20, 50, 100, 200, 500]\n",
    "    pen = ['l1', 'l2', 'None']\n",
    "    \n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True):\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        \n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "    \n",
    "    foldnum = 0\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "        train_sets[x], \n",
    "        train_set_labels[x], \n",
    "        test_size=0.11, \n",
    "        random_state=20160121)\n",
    "        for i in iters:\n",
    "            for p in pen: \n",
    "                oneVsRest = OneVsRestClassifier(linear_model.Perceptron(penalty=p, n_iter=i))\n",
    "                oneVsRest.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "        \n",
    "                columnname = \"Accuracy_pen\" + p + \"_numIter\" + str(i)\n",
    "                score = oneVsRest.score(bnb_validation, bnb_validation_labels)\n",
    "                fold_results_perc.loc[foldnum, columnname] = score\n",
    "                print(columnname + \": \" + str(score))\n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"-----------OneVsRestClassifier Perceptron results for accuracy-----------\")\n",
    "    print (fold_results_perc)\n",
    "    \n",
    "    print (\"--------------OneVsRestClassifier Perceptron mean accuracy----------------\")\n",
    "    print(fold_results_perc.mean())\n",
    "    \n",
    "ten_fold_oneVsRest_Perc(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Perceptron accuracy values------\n",
      "    Accuracy\n",
      "1   0.498079\n",
      "2   0.497072\n",
      "3   0.511361\n",
      "4   0.510658\n",
      "5   0.499930\n",
      "6   0.487187\n",
      "7   0.480159\n",
      "8   0.432748\n",
      "9   0.483439\n",
      "10  0.316936\n",
      "------Perceptron mean accuracy values------\n",
      "Accuracy    0.471757\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# The best value is for penalty = None, iterations: 200 : 0.500142\n",
    "\n",
    "fold_results_perc = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    # one fourth of the training data is for validation\n",
    "    foldnum+=1\n",
    "    oneVsRest = OneVsRestClassifier(linear_model.Perceptron(penalty='None', n_iter=200))\n",
    "    \n",
    "    oneVsRest.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_perc.loc[foldnum, 'Accuracy'] = oneVsRest.score(test_sets[x].values,\n",
    "                                                                 test_set_labels[x].values.ravel())\n",
    "    \n",
    "\n",
    "print (\"------Perceptron accuracy values------\")\n",
    "print (fold_results_perc)\n",
    "print (\"------Perceptron mean accuracy values------\")\n",
    "print(fold_results_perc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM : Not completing, takes forever to run\n",
    "\n",
    "from sklearn import svm\n",
    "    \n",
    "def ten_fold_svm(data, labels):\n",
    "    foldnum = 0\n",
    "    fold_results_ovr_est = pd.DataFrame()\n",
    "    fold_results_ada = pd.DataFrame()\n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True, random_state=20160202):\n",
    "        foldnum+=1\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        vec_mach = svm.SVC(kernel='rbf', random_state=20160202)\n",
    "        #print(tr_target)\n",
    "        vec_mach.fit(tr_data.values, tr_target[tr_target.columns.values[0]].values)\n",
    "        \n",
    "        fold_results_ada.loc[foldnum, 'Accuracy'] = vec_mach.score(te_data, te_target)\n",
    "        \n",
    "        predictions = vec_mach.predict_proba(te_data)\n",
    "        score = ndcg_score(te_target.as_matrix(), predictions, 5)\n",
    "        fold_results_ada.loc[foldnum, 'ndcg']  = score\n",
    "        print(score)\n",
    "            \n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"------svm accuracy values------\")\n",
    "    print (fold_results_ovr)\n",
    "    print (\"------svm mean accuracy values------\")\n",
    "    print(fold_results_ovr.mean())\n",
    "    \n",
    "ten_fold_svm(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
