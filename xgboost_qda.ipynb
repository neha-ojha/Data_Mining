{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Runs XGBoost, QDA, GBC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgbst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    2\n",
      "4    1\n",
      "Name: country_destination, dtype: int64\n",
      "   timestamp_first_active    gender       age  signup_method  signup_flow  \\\n",
      "0               -4.380020 -0.927300 -0.163283      -1.596552    -0.427798   \n",
      "1               -4.357961  1.058047  0.287705      -1.596552    -0.427798   \n",
      "2               -4.348661 -0.927300  2.317149       0.628333    -0.035009   \n",
      "3               -4.303076 -0.927300  0.738692      -1.596552    -0.427798   \n",
      "4               -4.283949 -0.927300  0.625945       0.628333    -0.427798   \n",
      "\n",
      "   language  affiliate_channel  affiliate_provider  first_affiliate_tracked  \\\n",
      "0 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "1 -0.141579           2.556797            0.251719                -0.798954   \n",
      "2 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "3 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "4 -0.141579          -0.582242           -0.468760                -0.798954   \n",
      "\n",
      "   signup_app      ...       Linux Desktop  Mac Desktop  Opera Phone  \\\n",
      "0   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "1   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "2   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "3   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "4   -0.359375      ...           -0.120933    -0.220553    -0.726855   \n",
      "\n",
      "     Tablet  Windows Desktop  Windows Phone  iPad Tablet    iPhone  iPodtouch  \\\n",
      "0 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "1 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "2 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "3 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "4 -0.062776        -0.196923      -0.707107    -0.108443 -0.133838  -0.153578   \n",
      "\n",
      "   secs_elapsed  \n",
      "0     -0.391996  \n",
      "1     -0.391996  \n",
      "2     -0.391996  \n",
      "3     -0.391996  \n",
      "4     -0.391996  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "Index([u'timestamp_first_active', u'gender', u'age', u'signup_method',\n",
      "       u'signup_flow', u'language', u'affiliate_channel',\n",
      "       u'affiliate_provider', u'first_affiliate_tracked', u'signup_app',\n",
      "       u'first_device_type', u'first_browser', u'created_year',\n",
      "       u'created_month', u'created_day', u'session_count', u'-unknown-_x',\n",
      "       u'booking_request', u'booking_response', u'click', u'data',\n",
      "       u'message_post', u'modify', u'partner_callback', u'submit', u'view',\n",
      "       u'-unknown-_y', u'Android App Unknown Phone/Tablet', u'Android Phone',\n",
      "       u'Blackberry', u'Chromebook', u'Linux Desktop', u'Mac Desktop',\n",
      "       u'Opera Phone', u'Tablet', u'Windows Desktop', u'Windows Phone',\n",
      "       u'iPad Tablet', u'iPhone', u'iPodtouch', u'secs_elapsed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the train and the test data \n",
    "# Run the preprocessing notebook to get the csv listed here\n",
    "train_users = pd.read_csv('train_users_merge_scale.csv')\n",
    "#test_users = pd.read_csv('test_users.csv')\n",
    "\n",
    "\n",
    "# Extracting labels from the train data\n",
    "train_users_labels = train_users.loc[:,'country_destination']\n",
    "print (train_users_labels.head(n=5))\n",
    "\n",
    "# Extracting attributes from the train data\n",
    "train_users_attrs = train_users.iloc[:,:-1]\n",
    "print(train_users_attrs.head(n=5))\n",
    "\n",
    "train_users = train_users_attrs\n",
    "print(train_users.columns)\n",
    "#train_users = train_users.drop(['id'], axis=1)\n",
    "train_users = train_users.drop(['Blackberry'], axis=1)\n",
    "train_users = train_users.drop(['Opera Phone'], axis=1)\n",
    "labels_df = pd.DataFrame(train_users_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def dcg_score(y_true, y_score, k=5):\n",
    "    \n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "#def ndcg_score(ground_truth, predictions, k=5):\n",
    "def ndcg_score(te_labels, predict, k):\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(12 + 1))\n",
    "    T = lb.transform(te_labels)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predict):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        if best == 0:\n",
    "            best = 0.000000001\n",
    "        score = float(actual) / float(best)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# NDCG Scorer function\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=True, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def folds_to_split(data,targets,train,test):\n",
    "    data_tr = pd.DataFrame(data).iloc[train]\n",
    "    data_te = pd.DataFrame(data).iloc[test]\n",
    "    labels_tr = pd.DataFrame(targets).iloc[train]\n",
    "    labels_te = pd.DataFrame(targets).iloc[test]\n",
    "    return [data_tr, data_te, labels_tr, labels_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827857150181\n",
      "0.827922539153\n",
      "0.831051357283\n",
      "0.830103425363\n",
      "0.830900322052\n",
      "0.829794146731\n",
      "0.829413793112\n",
      "0.825797729642\n",
      "0.829964795489\n",
      "0.832462151178\n",
      "------xbg accuracy values------\n",
      "    Accuracy      ndcg\n",
      "1   0.645414  0.827857\n",
      "2   0.643757  0.827923\n",
      "3   0.648817  0.831051\n",
      "4   0.647552  0.830103\n",
      "5   0.646521  0.830900\n",
      "6   0.645491  0.829794\n",
      "7   0.645866  0.829414\n",
      "8   0.637058  0.825798\n",
      "9   0.645303  0.829965\n",
      "10  0.650316  0.832462\n",
      "------xgb mean accuracy values------\n",
      "Accuracy    0.645610\n",
      "ndcg        0.829527\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model, cross_validation\n",
    "import xgboost\n",
    "\n",
    "def ten_fold_xgb(data, labels):\n",
    "    foldnum = 0\n",
    "    fold_results_ovr_est = pd.DataFrame()\n",
    "    fold_results_xgb = pd.DataFrame()\n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True):\n",
    "        foldnum+=1\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        xgb = xgbst.XGBClassifier(max_depth=5, learning_rate=0.3, n_estimators=25,\n",
    "                    objective='multi:softprob', subsample=0.5, colsample_bytree=0.7)\n",
    "        \n",
    "        #print(tr_target)\n",
    "        xgb.fit(tr_data.values, tr_target[tr_target.columns.values[0]].values)\n",
    "        #print(oneVsRest.estimators_)\n",
    "        \n",
    "        #perc.fit(heart_tr_data, heart_tr_target[0].values)\n",
    "        fold_results_xgb.loc[foldnum, 'Accuracy'] = xgb.score(te_data, te_target)\n",
    "        \n",
    "        predictions = xgb.predict_proba(te_data)\n",
    "        score = ndcg_score(te_target.as_matrix(), predictions, 5)\n",
    "        fold_results_xgb.loc[foldnum, 'ndcg']  = score\n",
    "        print(score)\n",
    "            \n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"------xbg accuracy values------\")\n",
    "    print (fold_results_xgb)\n",
    "    print (\"------xgb mean accuracy values------\")\n",
    "    print(fold_results_xgb.mean())\n",
    "    \n",
    "ten_fold_xgb(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83091005915: ndcg_dep4_est20_cs0.5\n",
      "0.830348312851: ndcg_dep4_est20_cs0.7\n",
      "0.830454820095: ndcg_dep4_est25_cs0.5\n",
      "0.829829458372: ndcg_dep4_est25_cs0.7\n",
      "0.830754727034: ndcg_dep4_est30_cs0.5\n",
      "0.830313406016: ndcg_dep4_est30_cs0.7\n",
      "0.830398149083: ndcg_dep5_est20_cs0.5\n",
      "0.830641409686: ndcg_dep5_est20_cs0.7\n",
      "0.830538736157: ndcg_dep5_est25_cs0.5\n",
      "0.830750932297: ndcg_dep5_est25_cs0.7\n",
      "0.830447993942: ndcg_dep5_est30_cs0.5\n",
      "0.831277747404: ndcg_dep5_est30_cs0.7\n",
      "0.830148420412: ndcg_dep6_est20_cs0.5\n",
      "0.830897545347: ndcg_dep6_est20_cs0.7\n",
      "0.830742977824: ndcg_dep6_est25_cs0.5\n",
      "0.830706467337: ndcg_dep6_est25_cs0.7\n",
      "0.830790206499: ndcg_dep6_est30_cs0.5\n",
      "0.830440402184: ndcg_dep6_est30_cs0.7\n",
      "0.829242283501: ndcg_dep4_est20_cs0.5\n",
      "0.829903461778: ndcg_dep4_est20_cs0.7\n",
      "0.830095713459: ndcg_dep4_est25_cs0.5\n",
      "0.829781317948: ndcg_dep4_est25_cs0.7\n",
      "0.830392484042: ndcg_dep4_est30_cs0.5\n",
      "0.82988891333: ndcg_dep4_est30_cs0.7\n",
      "0.829832133665: ndcg_dep5_est20_cs0.5\n",
      "0.830774939249: ndcg_dep5_est20_cs0.7\n",
      "0.830437469173: ndcg_dep5_est25_cs0.5\n",
      "0.830389297417: ndcg_dep5_est25_cs0.7\n",
      "0.830328707542: ndcg_dep5_est30_cs0.5\n",
      "0.830286494464: ndcg_dep5_est30_cs0.7\n",
      "0.83007440736: ndcg_dep6_est20_cs0.5\n",
      "0.830858461046: ndcg_dep6_est20_cs0.7\n",
      "0.830145048926: ndcg_dep6_est25_cs0.5\n",
      "0.8305711667: ndcg_dep6_est25_cs0.7\n",
      "0.830039273324: ndcg_dep6_est30_cs0.5\n",
      "0.830647255495: ndcg_dep6_est30_cs0.7\n",
      "0.830560457386: ndcg_dep4_est20_cs0.5\n",
      "0.830551362065: ndcg_dep4_est20_cs0.7\n",
      "0.83094855518: ndcg_dep4_est25_cs0.5\n",
      "0.831079833277: ndcg_dep4_est25_cs0.7\n",
      "0.831191550622: ndcg_dep4_est30_cs0.5\n",
      "0.830870133601: ndcg_dep4_est30_cs0.7\n",
      "0.830813655821: ndcg_dep5_est20_cs0.5\n",
      "0.831207714802: ndcg_dep5_est20_cs0.7\n",
      "0.831351203896: ndcg_dep5_est25_cs0.5\n",
      "0.83170730766: ndcg_dep5_est25_cs0.7\n",
      "0.831515671578: ndcg_dep5_est30_cs0.5\n",
      "0.831473275482: ndcg_dep5_est30_cs0.7\n",
      "0.830739644486: ndcg_dep6_est20_cs0.5\n",
      "0.831524234424: ndcg_dep6_est20_cs0.7\n",
      "0.831185013987: ndcg_dep6_est25_cs0.5\n",
      "0.830964001327: ndcg_dep6_est25_cs0.7\n",
      "0.831022168191: ndcg_dep6_est30_cs0.5\n",
      "0.831151958743: ndcg_dep6_est30_cs0.7\n",
      "0.825559463049: ndcg_dep4_est20_cs0.5\n",
      "0.827201845393: ndcg_dep4_est20_cs0.7\n",
      "0.825780522045: ndcg_dep4_est25_cs0.5\n",
      "0.827074628179: ndcg_dep4_est25_cs0.7\n",
      "0.826253062546: ndcg_dep4_est30_cs0.5\n",
      "0.827187406834: ndcg_dep4_est30_cs0.7\n",
      "0.826308671012: ndcg_dep5_est20_cs0.5\n",
      "0.826941798857: ndcg_dep5_est20_cs0.7\n",
      "0.826373370835: ndcg_dep5_est25_cs0.5\n",
      "0.827125704049: ndcg_dep5_est25_cs0.7\n",
      "0.826688302598: ndcg_dep5_est30_cs0.5\n",
      "0.826808470103: ndcg_dep5_est30_cs0.7\n",
      "0.82631110135: ndcg_dep6_est20_cs0.5\n",
      "0.82651664478: ndcg_dep6_est20_cs0.7\n",
      "0.8262092338: ndcg_dep6_est25_cs0.5\n",
      "0.826555219214: ndcg_dep6_est25_cs0.7\n",
      "0.826869974962: ndcg_dep6_est30_cs0.5\n",
      "0.826718915104: ndcg_dep6_est30_cs0.7\n",
      "0.829560586051: ndcg_dep4_est20_cs0.5\n",
      "0.829169394491: ndcg_dep4_est20_cs0.7\n",
      "0.83001619651: ndcg_dep4_est25_cs0.5\n",
      "0.829333283363: ndcg_dep4_est25_cs0.7\n",
      "0.830055552567: ndcg_dep4_est30_cs0.5\n",
      "0.829579416364: ndcg_dep4_est30_cs0.7\n",
      "0.82972489009: ndcg_dep5_est20_cs0.5\n",
      "0.830133640483: ndcg_dep5_est20_cs0.7\n",
      "0.830466017706: ndcg_dep5_est25_cs0.5\n",
      "0.830004687817: ndcg_dep5_est25_cs0.7\n",
      "0.830527556848: ndcg_dep5_est30_cs0.5\n",
      "0.829735537654: ndcg_dep5_est30_cs0.7\n",
      "0.829856019384: ndcg_dep6_est20_cs0.5\n",
      "0.829946409669: ndcg_dep6_est20_cs0.7\n",
      "0.830322821871: ndcg_dep6_est25_cs0.5\n",
      "0.830182350841: ndcg_dep6_est25_cs0.7\n",
      "0.830360028512: ndcg_dep6_est30_cs0.5\n",
      "0.829757596202: ndcg_dep6_est30_cs0.7\n",
      "0.827709064293: ndcg_dep4_est20_cs0.5\n",
      "0.827794985917: ndcg_dep4_est20_cs0.7\n",
      "0.827967222513: ndcg_dep4_est25_cs0.5\n",
      "0.827796768909: ndcg_dep4_est25_cs0.7\n",
      "0.828109830911: ndcg_dep4_est30_cs0.5\n",
      "0.827714321935: ndcg_dep4_est30_cs0.7\n",
      "0.8279067789: ndcg_dep5_est20_cs0.5\n",
      "0.827729347963: ndcg_dep5_est20_cs0.7\n",
      "0.827908459458: ndcg_dep5_est25_cs0.5\n",
      "0.827976015917: ndcg_dep5_est25_cs0.7\n",
      "0.828457845957: ndcg_dep5_est30_cs0.5\n",
      "0.828111555232: ndcg_dep5_est30_cs0.7\n",
      "0.828043328242: ndcg_dep6_est20_cs0.5\n",
      "0.828440876777: ndcg_dep6_est20_cs0.7\n",
      "0.828762843276: ndcg_dep6_est25_cs0.5\n",
      "0.828013384933: ndcg_dep6_est25_cs0.7\n",
      "0.828145675499: ndcg_dep6_est30_cs0.5\n",
      "0.827754620753: ndcg_dep6_est30_cs0.7\n",
      "0.827169847867: ndcg_dep4_est20_cs0.5\n",
      "0.828644964532: ndcg_dep4_est20_cs0.7\n",
      "0.827634934596: ndcg_dep4_est25_cs0.5\n",
      "0.82908123161: ndcg_dep4_est25_cs0.7\n",
      "0.827735517094: ndcg_dep4_est30_cs0.5\n",
      "0.829287582287: ndcg_dep4_est30_cs0.7\n",
      "0.828079675337: ndcg_dep5_est20_cs0.5\n",
      "0.828926365879: ndcg_dep5_est20_cs0.7\n",
      "0.828566035117: ndcg_dep5_est25_cs0.5\n",
      "0.828518645049: ndcg_dep5_est25_cs0.7\n",
      "0.828541634113: ndcg_dep5_est30_cs0.5\n",
      "0.82906732553: ndcg_dep5_est30_cs0.7\n",
      "0.827786899511: ndcg_dep6_est20_cs0.5\n",
      "0.827980891593: ndcg_dep6_est20_cs0.7\n",
      "0.828387243395: ndcg_dep6_est25_cs0.5\n",
      "0.828594555324: ndcg_dep6_est25_cs0.7\n",
      "0.82823952489: ndcg_dep6_est30_cs0.5\n",
      "0.828898608646: ndcg_dep6_est30_cs0.7\n",
      "0.82969585275: ndcg_dep4_est20_cs0.5\n",
      "0.829158079952: ndcg_dep4_est20_cs0.7\n",
      "0.829946338688: ndcg_dep4_est25_cs0.5\n",
      "0.829071519609: ndcg_dep4_est25_cs0.7\n",
      "0.830052418364: ndcg_dep4_est30_cs0.5\n",
      "0.829255476049: ndcg_dep4_est30_cs0.7\n",
      "0.829476803947: ndcg_dep5_est20_cs0.5\n",
      "0.830616370246: ndcg_dep5_est20_cs0.7\n",
      "0.829581287362: ndcg_dep5_est25_cs0.5\n",
      "0.830617823603: ndcg_dep5_est25_cs0.7\n",
      "0.830086931884: ndcg_dep5_est30_cs0.5\n",
      "0.830933826442: ndcg_dep5_est30_cs0.7\n",
      "0.82956525958: ndcg_dep6_est20_cs0.5\n",
      "0.8301559417: ndcg_dep6_est20_cs0.7\n",
      "0.829385506958: ndcg_dep6_est25_cs0.5\n",
      "0.829799746223: ndcg_dep6_est25_cs0.7\n",
      "0.829514247205: ndcg_dep6_est30_cs0.5\n",
      "0.829750135891: ndcg_dep6_est30_cs0.7\n",
      "0.832251011933: ndcg_dep4_est20_cs0.5\n",
      "0.831653110696: ndcg_dep4_est20_cs0.7\n",
      "0.832101815651: ndcg_dep4_est25_cs0.5\n",
      "0.831959235324: ndcg_dep4_est25_cs0.7\n",
      "0.832342340381: ndcg_dep4_est30_cs0.5\n",
      "0.832020206467: ndcg_dep4_est30_cs0.7\n",
      "0.83254485614: ndcg_dep5_est20_cs0.5\n",
      "0.832676363814: ndcg_dep5_est20_cs0.7\n",
      "0.832042102507: ndcg_dep5_est25_cs0.5\n",
      "0.832707478353: ndcg_dep5_est25_cs0.7\n",
      "0.832046590957: ndcg_dep5_est30_cs0.5\n",
      "0.832244626573: ndcg_dep5_est30_cs0.7\n",
      "0.831787630151: ndcg_dep6_est20_cs0.5\n",
      "0.833104627418: ndcg_dep6_est20_cs0.7\n",
      "0.83188987979: ndcg_dep6_est25_cs0.5\n",
      "0.832565616091: ndcg_dep6_est25_cs0.7\n",
      "0.831826657713: ndcg_dep6_est30_cs0.5\n",
      "0.8327831248: ndcg_dep6_est30_cs0.7\n",
      "0.832178707537: ndcg_dep4_est20_cs0.5\n",
      "0.83216485277: ndcg_dep4_est20_cs0.7\n",
      "0.832708044669: ndcg_dep4_est25_cs0.5\n",
      "0.832351640256: ndcg_dep4_est25_cs0.7\n",
      "0.832522480867: ndcg_dep4_est30_cs0.5\n",
      "0.832406363108: ndcg_dep4_est30_cs0.7\n",
      "0.832584880979: ndcg_dep5_est20_cs0.5\n",
      "0.832768665214: ndcg_dep5_est20_cs0.7\n",
      "0.83311328372: ndcg_dep5_est25_cs0.5\n",
      "0.833155589338: ndcg_dep5_est25_cs0.7\n",
      "0.833353263859: ndcg_dep5_est30_cs0.5\n",
      "0.832860485959: ndcg_dep5_est30_cs0.7\n",
      "0.832028887125: ndcg_dep6_est20_cs0.5\n",
      "0.833042175705: ndcg_dep6_est20_cs0.7\n",
      "0.832062267187: ndcg_dep6_est25_cs0.5\n",
      "0.833022840886: ndcg_dep6_est25_cs0.7\n",
      "0.832411863969: ndcg_dep6_est30_cs0.5\n",
      "0.833185634197: ndcg_dep6_est30_cs0.7\n",
      "------xbg accuracy values------\n",
      "    Accuracy_dep4_est20_cs0.5  ndcg_dep4_est20_cs0.5  \\\n",
      "1                    0.648022               0.830910   \n",
      "2                    0.644899               0.829242   \n",
      "3                    0.645987               0.830560   \n",
      "4                    0.638321               0.825559   \n",
      "5                    0.646555               0.829561   \n",
      "6                    0.641823               0.827709   \n",
      "7                    0.639457               0.827170   \n",
      "8                    0.644899               0.829696   \n",
      "9                    0.648637               0.832251   \n",
      "10                   0.649773               0.832179   \n",
      "\n",
      "    Accuracy_dep4_est20_cs0.7  ndcg_dep4_est20_cs0.7  \\\n",
      "1                    0.647501               0.830348   \n",
      "2                    0.646981               0.829903   \n",
      "3                    0.646792               0.830551   \n",
      "4                    0.642107               0.827202   \n",
      "5                    0.645751               0.829169   \n",
      "6                    0.642249               0.827795   \n",
      "7                    0.642817               0.828645   \n",
      "8                    0.644284               0.829158   \n",
      "9                    0.647691               0.831653   \n",
      "10                   0.650388               0.832165   \n",
      "\n",
      "    Accuracy_dep4_est25_cs0.5  ndcg_dep4_est25_cs0.5  \\\n",
      "1                    0.647454               0.830455   \n",
      "2                    0.647359               0.830096   \n",
      "3                    0.646650               0.830949   \n",
      "4                    0.640025               0.825781   \n",
      "5                    0.647596               0.830016   \n",
      "6                    0.642911               0.827967   \n",
      "7                    0.640592               0.827635   \n",
      "8                    0.645277               0.829946   \n",
      "9                    0.648401               0.832102   \n",
      "10                   0.651145               0.832708   \n",
      "\n",
      "    Accuracy_dep4_est25_cs0.7  ndcg_dep4_est25_cs0.7  \\\n",
      "1                    0.647312               0.829829   \n",
      "2                    0.646602               0.829781   \n",
      "3                    0.647880               0.831080   \n",
      "4                    0.641586               0.827075   \n",
      "5                    0.646508               0.829333   \n",
      "6                    0.642911               0.827797   \n",
      "7                    0.644094               0.829081   \n",
      "8                    0.643952               0.829072   \n",
      "9                    0.648826               0.831959   \n",
      "10                   0.650625               0.832352   \n",
      "\n",
      "    Accuracy_dep4_est30_cs0.5  ndcg_dep4_est30_cs0.5          ...            \\\n",
      "1                    0.647643               0.830755          ...             \n",
      "2                    0.647501               0.830392          ...             \n",
      "3                    0.647738               0.831192          ...             \n",
      "4                    0.641208               0.826253          ...             \n",
      "5                    0.648401               0.830056          ...             \n",
      "6                    0.643668               0.828110          ...             \n",
      "7                    0.641208               0.827736          ...             \n",
      "8                    0.645987               0.830052          ...             \n",
      "9                    0.649205               0.832342          ...             \n",
      "10                   0.651287               0.832522          ...             \n",
      "\n",
      "    Accuracy_dep6_est20_cs0.7  ndcg_dep6_est20_cs0.7  \\\n",
      "1                    0.649442               0.830898   \n",
      "2                    0.649394               0.830858   \n",
      "3                    0.648637               0.831524   \n",
      "4                    0.641160               0.826517   \n",
      "5                    0.648732               0.829946   \n",
      "6                    0.644426               0.828441   \n",
      "7                    0.642154               0.827981   \n",
      "8                    0.645892               0.830156   \n",
      "9                    0.651524               0.833105   \n",
      "10                   0.652376               0.833042   \n",
      "\n",
      "    Accuracy_dep6_est25_cs0.5  ndcg_dep6_est25_cs0.5  \\\n",
      "1                    0.648637               0.830743   \n",
      "2                    0.647738               0.830145   \n",
      "3                    0.647927               0.831185   \n",
      "4                    0.641302               0.826209   \n",
      "5                    0.649536               0.830323   \n",
      "6                    0.645467               0.828763   \n",
      "7                    0.643242               0.828387   \n",
      "8                    0.645419               0.829386   \n",
      "9                    0.648306               0.831890   \n",
      "10                   0.651666               0.832062   \n",
      "\n",
      "    Accuracy_dep6_est25_cs0.7  ndcg_dep6_est25_cs0.7  \\\n",
      "1                    0.648968               0.830706   \n",
      "2                    0.648921               0.830571   \n",
      "3                    0.647691               0.830964   \n",
      "4                    0.641681               0.826555   \n",
      "5                    0.649631               0.830182   \n",
      "6                    0.643952               0.828013   \n",
      "7                    0.643337               0.828595   \n",
      "8                    0.645751               0.829800   \n",
      "9                    0.650625               0.832566   \n",
      "10                   0.652234               0.833023   \n",
      "\n",
      "    Accuracy_dep6_est30_cs0.5  ndcg_dep6_est30_cs0.5  \\\n",
      "1                    0.648874               0.830790   \n",
      "2                    0.647596               0.830039   \n",
      "3                    0.647454               0.831022   \n",
      "4                    0.641965               0.826870   \n",
      "5                    0.649726               0.830360   \n",
      "6                    0.644804               0.828146   \n",
      "7                    0.643337               0.828240   \n",
      "8                    0.645135               0.829514   \n",
      "9                    0.648353               0.831827   \n",
      "10                   0.652139               0.832412   \n",
      "\n",
      "    Accuracy_dep6_est30_cs0.7  ndcg_dep6_est30_cs0.7  \n",
      "1                    0.648921               0.830440  \n",
      "2                    0.649063               0.830647  \n",
      "3                    0.648684               0.831152  \n",
      "4                    0.641728               0.826719  \n",
      "5                    0.648874               0.829758  \n",
      "6                    0.643952               0.827755  \n",
      "7                    0.643526               0.828899  \n",
      "8                    0.645940               0.829750  \n",
      "9                    0.651429               0.832783  \n",
      "10                   0.652707               0.833186  \n",
      "\n",
      "[10 rows x 36 columns]\n",
      "------xgb mean accuracy values------\n",
      "Accuracy_dep4_est20_cs0.5    0.644837\n",
      "ndcg_dep4_est20_cs0.5        0.829484\n",
      "Accuracy_dep4_est20_cs0.7    0.645656\n",
      "ndcg_dep4_est20_cs0.7        0.829659\n",
      "Accuracy_dep4_est25_cs0.5    0.645741\n",
      "ndcg_dep4_est25_cs0.5        0.829765\n",
      "Accuracy_dep4_est25_cs0.7    0.646030\n",
      "ndcg_dep4_est25_cs0.7        0.829736\n",
      "Accuracy_dep4_est30_cs0.5    0.646385\n",
      "ndcg_dep4_est30_cs0.5        0.829941\n",
      "Accuracy_dep4_est30_cs0.7    0.646361\n",
      "ndcg_dep4_est30_cs0.7        0.829852\n",
      "Accuracy_dep5_est20_cs0.5    0.645850\n",
      "ndcg_dep5_est20_cs0.5        0.829767\n",
      "Accuracy_dep5_est20_cs0.7    0.647208\n",
      "ndcg_dep5_est20_cs0.7        0.830242\n",
      "Accuracy_dep5_est25_cs0.5    0.646602\n",
      "ndcg_dep5_est25_cs0.5        0.830038\n",
      "Accuracy_dep5_est25_cs0.7    0.647482\n",
      "ndcg_dep5_est25_cs0.7        0.830295\n",
      "Accuracy_dep5_est30_cs0.5    0.647099\n",
      "ndcg_dep5_est30_cs0.5        0.830199\n",
      "Accuracy_dep5_est30_cs0.7    0.647525\n",
      "ndcg_dep5_est30_cs0.7        0.830280\n",
      "Accuracy_dep6_est20_cs0.5    0.645883\n",
      "ndcg_dep6_est20_cs0.5        0.829634\n",
      "Accuracy_dep6_est20_cs0.7    0.647374\n",
      "ndcg_dep6_est20_cs0.7        0.830247\n",
      "Accuracy_dep6_est25_cs0.5    0.646924\n",
      "ndcg_dep6_est25_cs0.5        0.829909\n",
      "Accuracy_dep6_est25_cs0.7    0.647279\n",
      "ndcg_dep6_est25_cs0.7        0.830098\n",
      "Accuracy_dep6_est30_cs0.5    0.646938\n",
      "ndcg_dep6_est30_cs0.5        0.829922\n",
      "Accuracy_dep6_est30_cs0.7    0.647482\n",
      "ndcg_dep6_est30_cs0.7        0.830109\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model, cross_validation\n",
    "import xgboost\n",
    "\n",
    "fold_results_xgb = pd.DataFrame()\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "def ten_fold_xgb_val(data, labels):\n",
    "    fold_results_ovr_est = pd.DataFrame()\n",
    "    \n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True):\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        \n",
    "        train_sets.append(tr_data)\n",
    "        train_set_labels.append(tr_target)\n",
    "        test_sets.append(te_data)\n",
    "        test_set_labels.append(te_target)\n",
    "    \n",
    "    depth = [4, 5, 6]\n",
    "    estimator = [20, 25, 30]\n",
    "    colsample = [0.5, 0.7]\n",
    "    foldnum = 0\n",
    "    for x in range(0, 10):\n",
    "        foldnum+=1\n",
    "        [bnb_train, bnb_validation, bnb_train_labels, bnb_validation_labels] = cross_validation.train_test_split(\n",
    "            train_sets[x], \n",
    "            train_set_labels[x], \n",
    "            test_size=0.11, \n",
    "            random_state=20160121)\n",
    "        for dep in depth:\n",
    "            for est in estimator:\n",
    "                for cs in colsample:\n",
    "                    xgb = xgbst.XGBClassifier(max_depth=dep, learning_rate=0.3, n_estimators=est,\n",
    "                                                objective='multi:softprob', subsample=0.5, colsample_bytree=cs)\n",
    "        \n",
    "        \n",
    "                    #print(tr_target)\n",
    "                    xgb.fit(bnb_train.values, bnb_train_labels[bnb_train_labels.columns.values[0]].values)\n",
    "                    #print(oneVsRest.estimators_)\n",
    "        \n",
    "                    #perc.fit(heart_tr_data, heart_tr_target[0].values)\n",
    "                    col_name1 = 'Accuracy_' + 'dep' +str(dep) +'_est' +str(est) + '_cs' +str(cs)\n",
    "                    fold_results_xgb.loc[foldnum, col_name1] = xgb.score(bnb_validation, bnb_validation_labels)\n",
    "        \n",
    "                    predictions = xgb.predict_proba(bnb_validation)\n",
    "                    score = ndcg_score(bnb_validation_labels.as_matrix(), predictions, 5)\n",
    "                    col_name2 = 'ndcg_' + 'dep' +str(dep) +'_est' +str(est)  + '_cs' +str(cs)\n",
    "                    fold_results_xgb.loc[foldnum, col_name2]  = score\n",
    "                    print(str(score) + ': ndcg_' + 'dep' +str(dep) +'_est' +str(est) + '_cs' +str(cs))\n",
    "            \n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"------xbg accuracy values------\")\n",
    "    print (fold_results_xgb)\n",
    "    print (\"------xgb mean accuracy values------\")\n",
    "    print(fold_results_xgb.mean())\n",
    "    \n",
    "ten_fold_xgb_val(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------xbg accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.647381  0.830642\n",
      "2   0.648630  0.830800\n",
      "3   0.651253  0.831373\n",
      "4   0.642773  0.827528\n",
      "5   0.642164  0.829708\n",
      "6   0.644647  0.829175\n",
      "7   0.645069  0.829802\n",
      "8   0.641602  0.828495\n",
      "9   0.650223  0.829953\n",
      "10  0.646381  0.829317\n",
      "------xgb mean accuracy and ndgc values------\n",
      "Accuracy    0.646012\n",
      "ndcg        0.829679\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Setting the attributes which do best in the validation set...\n",
    "\n",
    "fold_results_xgb2 = pd.DataFrame()\n",
    "foldnum = 0\n",
    "for x in range(0, 10):\n",
    "    \n",
    "    foldnum+=1\n",
    "    xgb = xgbst.XGBClassifier(max_depth=5, learning_rate=0.3, n_estimators=25,\n",
    "                              objective='multi:softprob', subsample=0.5, colsample_bytree=0.7)\n",
    "    xgb.fit(train_sets[x].values, train_set_labels[x].values.ravel())\n",
    "    \n",
    "    fold_results_xgb2.loc[foldnum, 'Accuracy'] = xgb.score(test_sets[x].values,\n",
    "                                                          test_set_labels[x].values.ravel())\n",
    "    \n",
    "    predictions = xgb.predict_proba(test_sets[x].values)\n",
    "    score = ndcg_score(test_set_labels[x].as_matrix(), predictions, 5)\n",
    "    fold_results_xgb2.loc[foldnum, 'ndcg']  = score\n",
    "    \n",
    "print (\"------xbg accuracy and ndcg values------\")\n",
    "print (fold_results_xgb2)\n",
    "print (\"------xgb mean accuracy and ndgc values------\")\n",
    "print(fold_results_xgb2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------QDA accuracy and ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.003701  0.021136\n",
      "2   0.002436  0.020786\n",
      "3   0.003982  0.026266\n",
      "4   0.003561  0.020837\n",
      "5   0.003092  0.020791\n",
      "6   0.003795  0.023865\n",
      "7   0.004076  0.023160\n",
      "8   0.003279  0.021676\n",
      "9   0.002764  0.019919\n",
      "10  0.004779  0.023424\n",
      "------QDA mean accuracy, ndcg values------\n",
      "Accuracy    0.003546\n",
      "ndcg        0.022186\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# QDA performs the worst among all the algorithms\n",
    "\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "fold_results_qda = pd.DataFrame()\n",
    "test_sets = []\n",
    "test_set_labels = []\n",
    "train_sets = []\n",
    "train_set_labels = []\n",
    "    \n",
    "def ten_fold_qda(data, labels):\n",
    "    foldnum = 0\n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True, random_state=20160202):\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "\n",
    "        foldnum+=1\n",
    "        quad_da = QDA()\n",
    "        #print(tr_target)\n",
    "        quad_da.fit(tr_data.values, tr_target[tr_target.columns.values[0]].values)\n",
    "        #print(oneVsRest.estimators_)\n",
    "        \n",
    "        fold_results_qda.loc[foldnum, 'Accuracy'] = quad_da.score(te_data, te_target)\n",
    "        \n",
    "        predictions = quad_da.predict_proba(te_data)\n",
    "        score = ndcg_score(te_target.as_matrix(), predictions, 5)\n",
    "        fold_results_qda.loc[foldnum, 'ndcg']  = score\n",
    "        #print(score)            \n",
    "                \n",
    "    #Now let's look at the results:\n",
    "    print (\"------QDA accuracy and ndcg values------\")\n",
    "    print (fold_results_qda)\n",
    "    print (\"------QDA mean accuracy, ndcg values------\")\n",
    "    print(fold_results_qda.mean())\n",
    "    \n",
    "ten_fold_qda(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808405508661\n",
      "0.823569179662\n",
      "0.823636935903\n",
      "0.823817813818\n",
      "0.825779005229\n",
      "0.824708311904\n",
      "0.825328238479\n",
      "0.821624199873\n",
      "0.829299174018\n",
      "0.827329062389\n",
      "------GradientBoostingClassifier accuracy,ndcg values------\n",
      "    Accuracy      ndcg\n",
      "1   0.630329  0.808406\n",
      "2   0.631717  0.823569\n",
      "3   0.633825  0.823637\n",
      "4   0.634809  0.823818\n",
      "5   0.638651  0.825779\n",
      "6   0.636449  0.824708\n",
      "7   0.638135  0.825328\n",
      "8   0.633076  0.821624\n",
      "9   0.642539  0.829299\n",
      "10  0.640993  0.827329\n",
      "------GradientBoostingClassifier mean accuracy,ndcg values------\n",
      "Accuracy    0.636052\n",
      "ndcg        0.823350\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def ten_fold_gbc(data, labels):\n",
    "    foldnum = 0\n",
    "    fold_results_ovr_est = pd.DataFrame()\n",
    "    fold_results_gbc = pd.DataFrame()\n",
    "    for train, test in cross_validation.KFold(len(data), n_folds=10, shuffle=True):\n",
    "        foldnum+=1\n",
    "        [tr_data, te_data,\n",
    "         tr_target, te_target] = folds_to_split(data, labels,train,test)\n",
    "        \n",
    "        gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.2, max_depth=5, random_state=0)\n",
    "        \n",
    "        #print(tr_target)\n",
    "        gbc.fit(tr_data.values, tr_target[tr_target.columns.values[0]].values)\n",
    "        #print(oneVsRest.estimators_)\n",
    "        \n",
    "        #perc.fit(heart_tr_data, heart_tr_target[0].values)\n",
    "        fold_results_gbc.loc[foldnum, 'Accuracy'] = gbc.score(te_data, te_target)\n",
    "        \n",
    "        predictions = gbc.predict_proba(te_data)\n",
    "        score = ndcg_score(te_target.as_matrix(), predictions, 5)\n",
    "        fold_results_gbc.loc[foldnum, 'ndcg']  = score\n",
    "        print(score)\n",
    "            \n",
    "    \n",
    "    #Now let's look at the results:\n",
    "    print (\"------GradientBoostingClassifier accuracy,ndcg values------\")\n",
    "    print (fold_results_gbc)\n",
    "    print (\"------GradientBoostingClassifier mean accuracy,ndcg values------\")\n",
    "    print(fold_results_gbc.mean())\n",
    "    \n",
    "ten_fold_gbc(train_users, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
